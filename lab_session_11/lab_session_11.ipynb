{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "yw6trru89_zZ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c057d71e237b4c5f397463ebb6d5a66",
     "grade": false,
     "grade_id": "cell-a474f0130611ac2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CSE204 - Introduction to Machine Learning - Lab Session 11: Dimensionality Reduction with AutoEncoders\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/adimajo/CSE204-2021/master/data/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSE204-2021](https://moodle.polytechnique.fr/course/view.php?id=12838) Lab session #11\n",
    "\n",
    "J.B. Scoggins - Adrien Ehrhardt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1a758ba7a8f3d6ce95a8c33e61f818e",
     "grade": false,
     "grade_id": "cell-0bc9dfa21277e476",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.datasets.mnist as mnist\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d1619948239c7aa406cbf56af5a0a68",
     "grade": false,
     "grade_id": "cell-3518fb9008a3444d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you will get hands-on experience with dimension reduction using Undercomplete Autoencoders. The goal of dimension reduction is to find a suitable transformation which converts a high-dimensional space into a smaller feature space, such that the important information is not lost, but the visualization and interpretability are easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "oTVnpKTn9_zf",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf58ca58674f45ab674437215c80e10c",
     "grade": false,
     "grade_id": "cell-300d5071dadb1cda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Recall the original MNIST Dataset and PCA decomposition\n",
    "\n",
    "We will reuse the MNIST digits dataset throughout this excercise. Recall that the original MNIST dataset provides $60{,}000$ 28x28 pixels (in grayscale) training images of hand-written digits 0-9. The images are labeled with integer values 0-9. The training set has become the _de facto_ image classification example due to its small size.\n",
    "\n",
    "In this exercise, we are not interested in classifying images of digits. Instead, we will think of the images as defining a 28x28 = 784 element feature space. In this context, we are interested in transforming the 784 parameters into a smaller set of transformed coordinates.  \n",
    "\n",
    "Recall from [lab_session_10](https://adimajo.github.io/CSE204-2021/lab_session_10/lab_session_10.html) that we used the keras datasets module to load the MNIST dataset, normalize it, and get to know how it is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "aol51DVe9_zg",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2264452de79eaada73d7d893ef03853",
     "grade": false,
     "grade_id": "cell-5578dbe82b796689",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_train_reshaped = x_train.reshape(60000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "765c9b0baf250d6426c657f74980d330",
     "grade": false,
     "grade_id": "cell-20f834cafc7b29ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The shape of `x_train`, `x_train_reshaped` and `y_train` correspond to what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d4dd3fad1cf278a4312ea55139501aa",
     "grade": false,
     "grade_id": "cell-9d6cdf3c9264786d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)  # 28 by 28 pixels\n",
    "print(x_train_reshaped.shape)  # flattened 28 by 28 = 784 \"pixels\"\n",
    "print(y_train.shape)  # one column of labels {0, ..., 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a03a5501a58476d4b9ab2f1e1633fbcb",
     "grade": false,
     "grade_id": "cell-aaa8fa0844b590da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can plot a few images using `matplotlib.pyplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9621fe508fbc4de7d8928ca4b929f642",
     "grade": false,
     "grade_id": "cell-e26784650bc5c284",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0, :, :])\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "3gkFxMdF9_zj",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32173a82810e13347aab0a12f801c9df",
     "grade": false,
     "grade_id": "cell-4c52185250d375c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Principal Component Analysis (PCA) - taken from [lab_session_10](https://adimajo.github.io/CSE204-2021/lab_session_10/lab_session_10.html)\n",
    "\n",
    "The goal of PCA is to perform an orthogonal transformation which converts a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables, called _principal components_. This can be thought of as fitting a $p$-dimensional ellipsoid to the observations.  \n",
    "\n",
    "Let's consider a dataset $X\\in R^{n\\times p}$, where $n$ is the number of observations and $p$ the number of variables.  PCA transforms $X$ into a new coordinate system (new variable set), such that the greatest variance in the data is captured in the first coordinate, and then the second, and so on.  More specifically, the transformed coordinates $T \\in R^{n\\times p}$ are written as a linear combination of the original dataset,\n",
    "\n",
    "$$ T = X W, $$\n",
    "\n",
    "where $W \\in R^{p\\times p}$ is the transformation matrix.  The first column of $W$, denoted as $w_1$, is constructed to maximize the variance of the transformed coordinates.\n",
    "\n",
    "$$ w_1 = \\underset{\\|w\\|=1}{\\operatorname{argmax}} \\sum_{i=1}^{n} (t_1)_i^2 = \\underset{\\|w\\|=1}{\\operatorname{argmax}} \\| X w \\|_2^2 = \\underset{\\|w\\|=1}{\\operatorname{argmax}} \\frac{w^T X^T X w}{w^T w} $$\n",
    "\n",
    "The ratio in the last term is known as the _Rayleigh quotient_.  It is well known that for the positive, semidefinite matrix $X^T X$, the largest value of the Rayleigh quotient is given as the largest eigenvalue of the matrix, where $w$ is eigenvector associated with that eigenvalue.\n",
    "\n",
    "The remaining columns of $W$ can be found by finding the the next orthogonal linear combination which maximizes the variance of the data, minus the previously transformed coordinates.\n",
    "\n",
    "$$ w_k = \\underset{\\|w\\|=1}{\\operatorname{argmax}} \\| (X - \\sum_{s=1}^{k-1} X w_s w_s^T) w \\|^2_2 $$\n",
    "\n",
    "Practically, the columns of $W$ are typically computed as the eigenvectors of $X^T X$ ordered by their corresponding eigenvalues in descending order.\n",
    "\n",
    "### Singular Value Decomposition\n",
    "\n",
    "The Singular Value Decomposition of a matrix $X \\in R^{n\\times p}$ is given as\n",
    "\n",
    "$$ X = U \\Sigma W^T, $$\n",
    "\n",
    "where $\\Sigma \\in R^{n\\times p}$ is a rectangular diagonal matrix of positive values known as the the singular values, of $X$, $\\sigma(X)$, and $U \\in R^{n\\times n}$ and $W \\in R^{p\\times p}$ are orthonormal matrices, whose columns are the left and right (respectively) singular vectors of the matrix $X$.  Using this decomposition, we can easily see that\n",
    "\n",
    "$$ X^T X = W \\hat{\\Sigma} W^T, $$\n",
    "\n",
    "where $\\hat{\\Sigma}$ is a square diagonal matrix of the squared singular values of $X$.  Comparing this to the eigenvalue decomposition of $X^T X = Q \\Lambda Q^T$, we see that the singular values of $X$ represent the square-root of the eigenvalues of $X^T X$, and the singular vectors of $X$ are simply the eigenvectors of $X^T X$.  Therefore, we can perform PCA on a data matrix $X$ by computing its right singular vector matrix, $W$.\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "We can reduce the dimensionality of our data by truncating the transformed variables to include only a subset of those variables with the highest variance.  For example, if we keep the first $L <= p$ variables, the reduced transformation reads\n",
    "\n",
    "$$ T_L = X W_L, $$\n",
    "\n",
    "where $W_L \\in R^{n\\times L}$ is the eigenvector matrix as before, but taking only the first $L$ columns.  This technique has been widely used to reduce the dimension of large-dimensioned datasets by accounting for the directions of largest variance in the data, while neglecting the other directions.  In addition, this can also be used to remove noise from a dataset, in which it is assumed that the noise accounts for a small degree of variance, compared to the true underlying parameterization.  Finally, using PCA to find the 2 highest varying parameters can also allow us to visualize a high-dimensional dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "yTRY_Mzw9_zs",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "636d6316cb8957ddf920a04b406e6696",
     "grade": false,
     "grade_id": "cell-cf39ce4971003a6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Reconstruct Images\n",
    "\n",
    "Now that we have an idea of how many principal components are necessary, let's use them to encode the images in a smaller set of features, which we can then decode to reconstruct the images from the lower-dimensional space.  Recall that based on the PCA transformation, we can compute the reconstructed images with\n",
    "\n",
    "$$ \\hat{X} = (X W_L) W_L^T $$\n",
    "\n",
    "Note that once we have computed the transformation matrix $W$, we essentially have a compression scheme to convert our images into a compressed format. From this perspective, using the first 5, 10, 30, and 100 principal components is equivalent to compressing the data at a rate of 156:1, 78:1, 26:1, and 8:1, respectively.  By contrast, JPEG image compression can obtain compression ratios of 23:1 with reasonable image quality, surpassing the quality of reconstructions with PCA. For that reason, PCA is not really used for image compression, but it has been used in a number of other fields, particularly in physics and engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "s_Yv_eRA9_zv",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9f8b3b162e1ca68f69503250626e608",
     "grade": false,
     "grade_id": "cell-07f9bbbd3f6f30ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Autoencoders\n",
    "\n",
    "In [lab_session_10](https://adimajo.github.io/CSE204-2021/lab_session_10/lab_session_10.html), we have seen that PCA was effective for producing a 2D or 3D representation of the points using \"interpretable\" principal components (linear combinations of the original features - see the `iris` dataset), but not very effective for image compression: we needed at least 100 coordinates (an 8:1 compression) to have credible digits.\n",
    "\n",
    "In this lab, we will devise a compression strategy using another method, autoencoders. **Autoencoders are neural networks which are trained to output their input** in such a way that they learn a reduced dimensional space of the input distribution. They are generally composed of two distinct \"layers\" (or two parts possibly composed of several layers). The first encodes the input space (encoder) and the second decodes the encoded space back to the original feature space (decoder). There are 3 basic types of autoencoders:\n",
    "1. __Undercomplete__ autoencoders work by constructing a network that has a hidden code layer that has fewer nodes than the input and output layers. After training, the smaller hidden layer will represent an encoding of the input onto a lower dimensional space.\n",
    "2. __Regularized__ autoencoders use various regularization terms in the loss function during training to constrict the space of the output. For example, sparse autoencoders add a sparsity regularization term in the loss to force as many nodes as possible in the hidden layers to be zero.\n",
    "3. __Variational__ autoencoders work slightly differently than the previous two. In this case, the autoencoder learns parameters that model the distribution of the input data in the encoder. The decoder is then used to reconstruct the output based on a random sample from this distribution. Their use is mainly directed towards data generation (*i.e.* generate credible faces).\n",
    "\n",
    "In this exercise, we will construct two **undercomplete** autoencoders and train them on the MNIST data as before with PCA.\n",
    "\n",
    "### First undercomplete autoencoder: dense *linear* decoder\n",
    "\n",
    "Recall that a dense linear neural network in the context of regression is similar to linear regression. Similarly, it is well known that an autoencoder with a linear decoder layer and a mean-squared-error loss function will learn the same feature space as PCA. Let's check this by creating a simple linear autoencoder.\n",
    "\n",
    "**Exercise 1:** Create a simple linear autoencoder.\n",
    "\n",
    "- Write a function which takes the `input_size` (denoted by $p$ previously) and the `code_size` (denoted by $L$ previously) and returns an autoencoder model using Keras.\n",
    "  - The autoencoder should be comprised of\n",
    "    - A dense **encoder** layer taking `input_size` inputs with `code_size` nodes and no activation function (implies an identity / linear activation function).\n",
    "    - An identity / linear **decoder** layer with `input_size` nodes.\n",
    "  - Compile the autoencoder using the Adam optimizer and MSE loss\n",
    "  - In addition to the **autoencoder** (the encoder + decoder), return the **encoder** as well.\n",
    "  \n",
    "*Hint*: use another `Model` which just takes the input and returns the output of the encoder layer (the encoder).  See the [functional API documentation](https://keras.io/getting-started/functional-api-guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "v0Or6sDL9_zw",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d57a6b97b359548dbfdbed9955e3c245",
     "grade": false,
     "grade_id": "cell-54929da0cc7fa40d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_autoencoder(input_size, code_size: int):\n",
    "    \"\"\"\n",
    "    Instanciate and compiles an autoencoder, returns both the autoencoder and just the encoder\n",
    "    (i.e. autoencoder except last layer)\n",
    "\n",
    "    :param int or tuple input_size: shape of the input samples\n",
    "    :param int code_size: dimension on which to project the original data\n",
    "    :return: autoencoder, encoder\n",
    "    \"\"\"\n",
    "    # autoencoder = ...\n",
    "    # encoder = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "930249bc180b3dd9d1006e506915c5a0",
     "grade": true,
     "grade_id": "cell-70238992fb2b5118",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aed586530a8ceb9d34d97608434f4070",
     "grade": true,
     "grade_id": "cell-397e844f3e3f22b8",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e77d150b48ff14cfe7fbaa804fe3911a",
     "grade": true,
     "grade_id": "cell-6c4464b8982087de",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "b50NdnEy9_zz",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0650168e1011fd93b346bfa59bdfe1cf",
     "grade": false,
     "grade_id": "cell-d09ada99d4caca94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Exercise 2:** Train the linear autoencoder.\n",
    "\n",
    "- Using your function, create a linear autoencoder with the correct `input_size` and a `code_size` of 2 (similar to what we did with PCA).\n",
    "- Train the model using the MNIST data as input and output for at least 7 epochs.\n",
    "- Plot the history of the loss versus the epoch number to make sure training is basically complete.\n",
    "\n",
    "*Hint*: the history of the loss is silently returned by the call to [`fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "vHMQttV29_zz",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "045a45574e4b86ff3eedfb0deb046c5f",
     "grade": false,
     "grade_id": "cell-11f8343be4f3dbd3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# linear_ae, linear_encoder = linear_autoencoder(input_size, code_size)\n",
    "# history = ...\n",
    "# plt.plot(history...)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "091918c287bb9d3ff92afb6963ecc9d6",
     "grade": true,
     "grade_id": "cell-1b9b382a8e609540",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "h-QCBJJF9_z2",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52011446f981a305900b178b08179e53",
     "grade": false,
     "grade_id": "cell-5903ff45970a6778",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Exercise 3:** Use the trained encoder to project the MNIST data to 2 features.\n",
    "- Plot the two components in a scatter similar to the one we produced in [lab_session_10](https://adimajo.github.io/CSE204-2021/lab_session_10/lab_session_10.html) for PCA.\n",
    "- How does the scatter plot compare to the one we made with PCA?\n",
    "\n",
    "Recall that this autoencoder should learn the same vector space as PCA, though it will not learn the exact same transformation (could be rotated, scaled, etc.). Similarly to [lab_session_10](https://adimajo.github.io/CSE204-2021/lab_session_10/lab_session_10.html), you might want to draw fewer points to be able to see something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "ZPqPGscZ9_z4",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6e4bb4b06992a933cdcb80600ed70a1",
     "grade": true,
     "grade_id": "cell-b3b4309b82b46ee8",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ... linear_encoder...(...)  # <- TO UNCOMMENT AND COMPLETE\n",
    "# plt.scatter(...)  # <- TO UNCOMMENT AND COMPLETE\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9eb1bd1cc2dc70432b72496d3f9f71a4",
     "grade": true,
     "grade_id": "cell-b55102a3a94b8688",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "CEIYONHn9_z6",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "470c3a1c98abd44aa8451de14a76044e",
     "grade": false,
     "grade_id": "cell-4bfba8f586246378",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Second undercomplete autoencoder: *nonlinear* decoder\n",
    "\n",
    "We saw in the previous section that linear decoders and MSE loss produce the same result as PCA.  Therefore, we can see nonlinear decoders as a nonlinear generalization of PCA. By allowing nonlinear transformations, we should be able to increase the \"expressiveness\", or the quality of the representation / separation of our reduced variables.  \n",
    "\n",
    "**Exercise 4:** Create a nonlinear autoencoder.\n",
    "\n",
    "- Copy your linear autoencoder function into `nonlinear_autoencoder`.\n",
    "- Add a dense hidden layer in between the encoder output and decoder output layers. Give the hidden layer `input_size` / 2 nodes and use ReLU activations for both the encoder and the additional hidden layer.\n",
    "- Keep the same optimizer and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "W1WyYdo69_z7",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5e56b5f5fc4a618a13e5d9a1490a5bd",
     "grade": false,
     "grade_id": "cell-d52c5f608a0fd7bf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def nonlinear_autoencoder(input_size, code_size: int):\n",
    "    \"\"\"\n",
    "    Instanciate and compiles an autoencoder, returns both the autoencoder and just the encoder\n",
    "\n",
    "    :param int or tuple input_size: shape of the input samples\n",
    "    :param int code_size: dimension on which to project the original data\n",
    "    :return: autoencoder, encoder\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88e2820722b57a813ca6a99b00ede4d1",
     "grade": true,
     "grade_id": "cell-8c5b23709486ba56",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "Psakm9VQ9_z8",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a17d303621c36ed58d6f611b6e11cf4",
     "grade": false,
     "grade_id": "cell-5e9f2e6f5f0bf38a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Exercise 5:** Train nonlinear reduced model.\n",
    "- Create the nonlinear AE using 2 representation features as with the linear model.\n",
    "- Train as before for at least 7 epochs and plot the loss history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "rmvB9j-a9_z9",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23f6fb0c2ca03b71137e549d07ff151c",
     "grade": true,
     "grade_id": "cell-8358ad3d04c93336",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# nonlinear_ae, nonlinear_encoder = nonlinear_autoencoder(input_size, code_size)\n",
    "# history = ...\n",
    "# plt.plot(...)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "_GqBlgjb9_z_",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec1098b5b23d9f85b7600f393a5cac0f",
     "grade": false,
     "grade_id": "cell-0a9da84a07bdf2ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Exercise 6:** Plot the scatter plot of the reduced variables (similar to Exercise 3).\n",
    "\n",
    "- What can you say about grouping of points using the nonlinear model?  Does it seem to cluster the digits better than with the linear one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "tHTJyBo39_0A",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76f3674a24daa4e07fe0903f7318c2e0",
     "grade": true,
     "grade_id": "cell-99c8a116eacfd79d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ... nonlinear_encoder...(...)\n",
    "# plt.scatter(...)  # <- TO UNCOMMENT AND COMPLETE\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ed5738b3fe0ef5c62e683516a154ad4",
     "grade": true,
     "grade_id": "cell-97ce58ec17cc0949",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "5veU70oR9_0C",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0df8cb1118cda4d016ef5ced4ae98be3",
     "grade": false,
     "grade_id": "cell-92daeb63df0d1092",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Reconstruct Images\n",
    "\n",
    "**Exercise 7:** Use the autoencoders to produce reconstructed images from the MNIST data as we did with PCA.\n",
    "- Train linear and nonlinear autoencoders on the MNIST data using a `code_size` of 15.\n",
    "- Compare the loss histories of the training for both models on the same plot. What does this tell you about the expressiveness, i.e. the quality of the reconstruction of the two models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "Hdgn_1Im9_0C",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f1b29cb331548f8d26f929687c28efe",
     "grade": true,
     "grade_id": "cell-56d721348a8462f3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train autoencoder models, get history of losses\n",
    "\n",
    "# Plot the losses\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plt.legend(['linear', 'nonlinear']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "621519290a015d83b389c248dc4cffe5",
     "grade": false,
     "grade_id": "cell-5339695b7abf3fb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It should be clear now that you were able to achieve a lower loss, all else equal, using the nonlinear autoencoder. Let's see what this will yield in terms of reconstruction quality / compression capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "sUQW2Pok9_0F",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb15db56d5b8c74ff35d42297fe2e2fa",
     "grade": false,
     "grade_id": "cell-a0246d51ca20d914",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Exercise 8:** Use the two AEs to produce reconstructed images (see [lab_session_10](https://adimajo.github.io/CSE204-2021/lab_session_10/lab_session_10.html)).\n",
    "- Generate a grid of images\n",
    "  - The first row should contain the first 5 images in the MNIST set as before.\n",
    "  - The second row should contain their reconstruction using the linear model.\n",
    "  - The third row should contain the reconstructions using the nonlinear model.\n",
    "- How well do each of the models reproduce the images? \n",
    "- How do they compare to the PCA reconstructions?\n",
    "\n",
    "Recall from Exercise 7 that we used a `code_size` of 15, achieving a 784:15 $\\approx$ 52:1 compression! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "MRAaR5oJ9_0G",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ce6a2b53c3dd9517dbd0b5f31decaad",
     "grade": true,
     "grade_id": "cell-cc894327bb4bdb31",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "676d93f94241c9c0f2b10b2c15dfcecc",
     "grade": true,
     "grade_id": "cell-ca9b3128c147d1bf",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbae8b2870275f79cdd282a4b4eb66e0",
     "grade": false,
     "grade_id": "cell-92fd97f9696a37ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Beyond this lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7a49f90dbda3551e9f17a46667f1cb3",
     "grade": false,
     "grade_id": "cell-89d8000562d5acc5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "* See the wikipedia entry for [JPEG](https://en.wikipedia.org/wiki/JPEG#Encoding) ([discrete cosine basis](https://en.wikipedia.org/wiki/Discrete_cosine_transform)) and [JPEG2000](https://en.wikipedia.org/wiki/JPEG_2000) ([wavelet transform basis](https://en.wikipedia.org/wiki/Wavelet_transform))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89a589a974653464614f179891d4ff00",
     "grade": false,
     "grade_id": "cell-5b84fdcc3a5e6f74",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "* Copy / paste the `nonlinear_autoencoder` function above and complexify the model (*e.g.* more layers, different activation functions, `Dropout`) and rerun Exercices 7-8: were you able to achieve even better image reconstruction quality?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copie de Lab10.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/jbscoggi/teaching/blob/master/Polytechnique/CSE204/Lab10.ipynb",
     "timestamp": 1576227388467
    }
   ]
  },
  "kernelspec": {
   "display_name": "CSE204",
   "language": "python",
   "name": "cse204"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
