{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84f539d93100a24a9a703103ccc9bbd9",
     "grade": false,
     "grade_id": "cell-144326b3318169b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CSE204 - Introduction to Machine Learning - Lab Session 5 - Exam\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/adimajo/CSE204-2021/master/data/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSE204-2021](https://moodle.polytechnique.fr/enrol/index.php?id=12838) Lab session #05 - Lab Exam\n",
    "\n",
    "Théo Lacombe - Adrien Ehrhardt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "532ab6b3247143bb23c9e487a6247334",
     "grade": false,
     "grade_id": "cell-a5ae1c6d33edd6b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Overall presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "629d938ac15bf5673d35ac9b40425dd2",
     "grade": false,
     "grade_id": "cell-2e7d7c15417b6cfb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This lab is composed of 3 exercises, granting up to 6, 7.5 and 5.5 points, and 2 additional exercises (2 pts each - the final grade is over 20, so there are bonus points).\n",
    "\n",
    "These exercises are independant.\n",
    "\n",
    "There are examples of automatic tests that are run against your code. **They are not exhaustive nor sufficient** (we will run other - hidden - tests), **but they are necessary**: they have to pass, otherwise you can be sure *not* to get the points.\n",
    "\n",
    "You **cannot** use past lab sessions' solutions, nor Google anything. The exam is open book w.r.t. the lectures. Some help and hints are provided for each question (e.g. which function to use and how), and you can also use the `help(...)` function.\n",
    "\n",
    "- **Do not** delete any pre-existing cell (you can create and delete your own cells for testing).\n",
    "- **Do not** change the type (Markdown / Code / ...) of any pre-existing cell.\n",
    "- Run the notebook with the **CSE204** kernel - if you didn't install it beforehand (come on, it's the $5^{th}$ lab!), proceed at your own risk.\n",
    "- **Do not** rename the file when uploading your work on Moodle.\n",
    "- **Do not** edit the notebook's or a cell's metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b818c88a94b465164de465ad9685c3a",
     "grade": false,
     "grade_id": "cell-2369821571c3dd70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c338f272d8949d9fe652d96cd4118ea",
     "grade": false,
     "grade_id": "cell-9928043d5ac7241c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45f04478a37cf24ac4092af102932233",
     "grade": false,
     "grade_id": "cell-846591d2040401d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gen_d_regression_samples(n_samples: int = 50, p: int = 3,\n",
    "                             reg_type: str = 'linear', seed: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate p-dimensional regression samples\n",
    "\n",
    "    :param int n_samples: number of samples to draw\n",
    "    :param int p: dimension of the inputs\n",
    "    :param str reg_type: either linear or polynomial\n",
    "    :param int seed: random seed\n",
    "    :return: dataframe with (x_i)_1^p and y\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    np.random.seed(1) \n",
    "    if reg_type=='linear':\n",
    "        x = np.random.uniform(low=1.5, high=3, size=(n_samples, p))\n",
    "        y = np.sum(x, axis=1) + np.random.normal(scale=0.15, size=n_samples)\n",
    "        df = pd.DataFrame(x, columns=['x' + str(i) for i in range(p)])\n",
    "        df['y'] = y\n",
    "    else:\n",
    "        x = np.random.uniform(low=0, high=1.5, size=(n_samples, p))\n",
    "        y = np.sum(x, axis=1) + np.sum(x**2, axis=1) + np.random.normal(scale=0.1, size=n_samples)\n",
    "        df = pd.DataFrame(x, columns=['x' + str(i) for i in range(p)])\n",
    "        df['y'] = y\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "819a2f66cc8f49e994cc0584a35b3bdc",
     "grade": false,
     "grade_id": "cell-63b9615d29198042",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 1: Linear regression and its extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7b32b7795f80e386233de4ec5283709",
     "grade": false,
     "grade_id": "cell-f43b47557b44cd94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We briefly recall the framework of linear regression.\n",
    "\n",
    "We suppose data is generated following: $y = f(x) + \\epsilon = \\theta_0 + \\theta_1 x^{(i)}_1 + \\dots + \\theta_p x^{(i)}_p + \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, \\sigma)$.\n",
    "\n",
    "Consider observations $\\boldsymbol{X} = \\left( x^{(1)} \\dots x^{(n)} \\right)^T$, with $x^{(i)} \\in \\mathbb{R}^p$, and labels $\\boldsymbol{y} = \\left( y^{(1)} \\dots y^{(n)} \\right)^T$.\n",
    "Given an observation $x^{(i)}$, and a vector $\\theta = (\\theta_0 \\dots \\theta_p)^T$, we produce an estimation $\\hat{y}^{(i)}$ of $y^{(i)}$ of the following form:\n",
    "$$ \\hat{y}^{(i)} = \\hat{f}(x) = \\theta_0 + \\theta_1 x^{(i)}_1 + \\dots + \\theta_p x^{(i)}_p.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f71c9487242cac6c048885d3099d25f",
     "grade": false,
     "grade_id": "cell-098c355f0b11bd9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Vanilla linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9be7b9cce372a4c86ef3cd25548ce877",
     "grade": false,
     "grade_id": "cell-a6e2b330e058b93a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's sample some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89889d38937998e493a8d4480b2a1e22",
     "grade": false,
     "grade_id": "cell-0335caff069e24e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "linear_df = gen_d_regression_samples(n_samples=200, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e4aa048a8685bf0bc1569b16a31dcb1",
     "grade": false,
     "grade_id": "cell-128bacfa7998346b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "linear_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82b823e65ab64447a6ea89ef15da963b",
     "grade": false,
     "grade_id": "cell-520f63f1436d1208",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 1: (1pt)** Implement a function `pred` which, given an observation `x` of shape `p` and a vector `theta` of shape `p+1`, returns a predicted value `y_hat`, given the mathematical expression above.\n",
    "\n",
    "_Hint:_ Do not forget the constant term $\\theta_0$ - hence the different shapes for `x` and `theta` - for example, `1` can be \"added\" to `x` inside the `pred` function if you wish to use vector / matrix multiplication.\n",
    "\n",
    "_Python hints_ : \n",
    "- You can use `np.concatenate((A, B))` to concatenate two numpy arrays. \n",
    "- You can use `np.dot(A,B)`, or equivalently `A.dot(B)` to compute the matrix-matrix (or matrix-vector) $A \\cdot B$ product between two numpy arrays `A` and `B`. Equivalently, you can use matrix multiplication with vectors (either `A @ B` or `np.matmul(A, B)`), but beware of the shapes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16f3d077ac7366440211fdaa7be591dd",
     "grade": false,
     "grade_id": "cell-84f5f72dbc776574",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pred(x: np.array, theta: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Implementation of f_hat\n",
    "\n",
    "    :param numpy.array x: a sample with p features\n",
    "    :param numpy.array theta: a vector of (p + 1) parameters\n",
    "    :return: y_hat\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "daab4ba018c60084a808c97bd442b73d",
     "grade": true,
     "grade_id": "cell-6d1e33980f79e436",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert pred(np.array([0]), np.array([0,0])) == 0  # This computes y_hat = 0 + 0 * 0 which should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aca472d0b275971b61c9097dd784e427",
     "grade": false,
     "grade_id": "cell-2027056bed52afea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Recall that all samples are gathered in matrices $\\boldsymbol{X}$ and $\\boldsymbol{y}$, where\n",
    "$$\n",
    "\\boldsymbol{X} = \\begin{pmatrix} x_1^{(1)} & \\dots & x_p^{(1)} \\\\ \\vdots & \\vdots & \\vdots \\\\ x_1^{(n)} & \\dots & x_p^{(n)} \\end{pmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "\\boldsymbol{y} = \\begin{pmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "We propose a loss function $E(\\theta) := \\| \\boldsymbol{y} - \\boldsymbol{X}^T \\theta\\|^2_2$ which is the sum of squared errors (differences between the true and predicted value for each $x$ in $\\boldsymbol{X}$), or equivalently (in vector form) the norm of the vector of each sample's prediction error $y^{(i)} - \\hat{y}^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97dd9a0964e187d22eb2ca955aadb1e1",
     "grade": false,
     "grade_id": "cell-bd3c3d48859e0a80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2: (1pt)** Implement a function `E` which, given `X, y, theta` computes the error $E(\\theta)$ defined above (you can use the `pred` function defined in question 1).\n",
    "\n",
    "_Python hint:_ You can compute the norm of a vector `A` using `np.linalg.norm(A)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e6fb2db91b78cbf4baf92bae3c92eca",
     "grade": false,
     "grade_id": "cell-4b253d188a4f3f29",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def E(X: np.array, y: np.array, theta: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Implementation of the error function: sum of squared errors\n",
    "\n",
    "    :param numpy.array X: design matrix of shape N x p\n",
    "    :param numpy.array y: response values of shape N\n",
    "    :param numpy.array theta: coefficient of shape (p+1)\n",
    "    :return: evaluation of the error function\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fce6121bf76254d144009d5d1e24c245",
     "grade": true,
     "grade_id": "cell-51418e1b2f4adf49",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test for a particular value of E\n",
    "assert int(E(linear_df[['x0', 'x1']].to_numpy(), linear_df['y'], np.array([0,0,0]))) == 4180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find an optimal vector $\\theta^*$ so that the loss made by using $\\hat{y}^{(i)}$ to approximate $y^{(i)}$ is small.\n",
    "With vector notations, it reads:\n",
    "$$\\theta^* \\in \\mathrm{argmin} (E(\\theta)).$$\n",
    "\n",
    "Since $E(\\theta)$ is convex (the square is convex and everything else is linear), $\\theta^\\star$ exists and is unique (you can also calculate the Hessian matrix to convince yourself).\n",
    "\n",
    "Recall that to find $\\theta^\\star$, a straightforward solution is to differentiate $E(\\theta)$ so as to obtain its gradient, set it to 0 and solve the resulting equation. \n",
    "\n",
    "This yields a closed form equation. The optimal $\\theta^\\star$ is given by:\n",
    "$$ \\theta^\\star = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b8a27317dd91d2f34f79b4da6768d6e",
     "grade": false,
     "grade_id": "cell-613c9ada1bd281f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 3: (1pt)** Implement a function `linreg` which, given `X` and `y`, returns the optimal vector `theta_star` as suggested above.\n",
    "\n",
    "_Python hints_ : \n",
    "- You can use `np.linalg.inv(A)` to compute the inverse of a (non-singular) **square** matrix. \n",
    "- You can use `np.transpose(A)` or equivalently `A.T` to compute the transpose of a numpy array `A`. \n",
    "- You can use `np.dot(A,B)`, or equivalently `A.dot(B)` to compute the matrix-matrix (or matrix-vector) $A \\cdot B$ product between two numpy arrays `A` and `B`. Equivalently, you can use matrix multiplication with matrix and vectors (either `A @ B` or `np.matmul(A, B)`), but beware of the shapes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "432ad267081e3aac0fc0c1d6656849dd",
     "grade": false,
     "grade_id": "cell-fbe43c45a99b9576",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linreg(X: np.array, y: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Compute linear regression coefficient with OLS\n",
    "\n",
    "    :param numpy.array X: design matrix of shape N x (p+1)\n",
    "    :param numpy.array y: response vector of shape N\n",
    "    :return: coefficients of shape (p+1)\n",
    "    :rtype: numpy.array\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d26f356132c3544065a5f6a01735508d",
     "grade": false,
     "grade_id": "cell-3b8eaf1f7b644bd1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your code with the following code - please **do not** be satisfied with the fact that it runs... Figure out what is printed, and whether it's correct - no qualitative answer expected though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9561d5a1fe3a0b522690260bfd0fa98b",
     "grade": true,
     "grade_id": "cell-6fb987efac7bf1ac",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Transform columns of features into numpy array and add a column of 1\n",
    "design_matrix = np.concatenate((np.ones(linear_df.shape[0]).reshape(-1, 1),\n",
    "                                linear_df[['x0', 'x1']].to_numpy()), axis=1)\n",
    "\n",
    "# Linear regression coefficient\n",
    "theta = linreg(design_matrix,\n",
    "               linear_df['y'])\n",
    "\n",
    "print(\"Coefficients:\\t\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afdf3abfeffc41a4cac5f82abea2525e",
     "grade": false,
     "grade_id": "cell-8885b13829eefb4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Another way to verify the validity of your implementation is to plot your solution, with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b63de3daa1bbc6031d3e316a17cbbe8b",
     "grade": false,
     "grade_id": "cell-eac91c61ac0e6017",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a figure\n",
    "fig = plt.figure(dpi=150)\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Plotting our data points\n",
    "ax.scatter(linear_df[['x0']].to_numpy(),\n",
    "           linear_df[['x1']].to_numpy(),\n",
    "           linear_df['y'].to_numpy())\n",
    "\n",
    "# Creating a mesh to draw the linear regression hyperplane\n",
    "x0_surf = np.arange(linear_df[['x0']].min().values, linear_df[['x0']].max().values, 0.1)\n",
    "x1_surf = np.arange(linear_df[['x1']].min().values, linear_df[['x1']].max().values, 0.1)\n",
    "x0_surf, x1_surf = np.meshgrid(x0_surf, x1_surf)\n",
    "exog = pd.core.frame.DataFrame({'x0': x0_surf.ravel(), 'Radio': x1_surf.ravel()})\n",
    "\n",
    "# Prediction on the mesh\n",
    "out = [pred(x, theta) for x in exog.to_numpy()]\n",
    "\n",
    "# Plotting the hyperplane\n",
    "ax.plot_surface(x0_surf,\n",
    "                x1_surf,\n",
    "                np.array(out).reshape(x0_surf.shape),\n",
    "                rstride=1,\n",
    "                cstride=1,\n",
    "                color='red',\n",
    "                alpha = 0.4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "435bdd4aa780e5fd696dd3d19ca826d0",
     "grade": false,
     "grade_id": "cell-ce3af95b146f5e4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e34651ccb930eed16c6a5f97713d953",
     "grade": false,
     "grade_id": "cell-f10f624abcee3147",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We consider the specific case where the observations $x^{(i)}$ are real-valued (that is $p=1$). For an integer $k$ fixed, and a vector $\\theta = (\\theta_0 \\dots \\theta_k)^T$, we propose to estimate $y^{(i)}$ in the following way:\n",
    "$$ \\hat{y}^{(i)} = \\theta_0 + \\theta_1 x^{(i)} + \\theta_2 (x^{(i)})^2 + \\dots + \\theta_k (x^{(i)})^k. $$\n",
    "Here, $(x^{(i)})^j$ means the $j$-th power of $x^{(i)}$.\n",
    "\n",
    "As in \"standard\" linear regression, the goal is to find a $\\theta^*$ so that \n",
    "$ \\sum_{i=1}^n (y^{(i)} - \\hat{y}^{(i)})^2 $\n",
    "is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d523763dc080a35008f8b47ae7279a12",
     "grade": false,
     "grade_id": "cell-fc5de4c2c79cdd8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We consider a dataset of $n = 100$ pairs of form (observation, label), that is split into a _train set_ of size $80$ and a _test set_ of size $20$. These data are loaded using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b2c50cc78f5c749148d2b86c5eb72de",
     "grade": false,
     "grade_id": "cell-b32e4209139b0a07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "polynomial_df = gen_d_regression_samples(n_samples=100, p=1, reg_type='polynomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3443c8b0542fdc883e960e7171169e85",
     "grade": false,
     "grade_id": "cell-60c54d94b6a3ff5b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "polynomial_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a268f2fafa906615108cb914c57cfce9",
     "grade": false,
     "grade_id": "cell-23e5aad911481833",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = polynomial_df['x0'].to_numpy()\n",
    "y = polynomial_df['y'].to_numpy()\n",
    "\n",
    "print(X[:5], \"\\n\")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec161f731626cc2c4087b0d555f4a4a0",
     "grade": false,
     "grade_id": "cell-9a73b80210cdc967",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = sk.model_selection.train_test_split(X, \n",
    "                                                                       y, \n",
    "                                                                       test_size=1/5, \n",
    "                                                                       random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c14bd4ed23f41b7c8732254d7180de33",
     "grade": false,
     "grade_id": "cell-55f668a740eb5215",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 4 (1pt):** Write a function `polynomial_expand` which, given a **real valued** vector $x \\in \\mathbb{R}^n$ and an integer $k$, returns the $n \\times (k+1)$ matrix\n",
    "$$X = \\begin{pmatrix} 1 & x^{(1)} & \\dots & (x^{(1)})^k \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & x^{(n)} & \\dots & (x^{(n)})^k \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9d4d0b8f8a95831be0707701ac55606",
     "grade": false,
     "grade_id": "cell-ed647fbde310cbc3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def polynomial_expand(x: np.array, k: int = 5) -> np.array:\n",
    "    \"\"\"\n",
    "    Compute design matrix X as above\n",
    "\n",
    "    :param numpy.array x: vector of inputs of shape n\n",
    "    :param int k: polynomial degree\n",
    "    :return: design matrix X (as above) of shape n x (k+1)\n",
    "    :rtype: numpy.array\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ef4f957f690fd914f72f566e9b23798",
     "grade": true,
     "grade_id": "cell-9e9532bd64baaca9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert polynomial_expand(x_train, 10).shape == (80, 11)  # correct (n, k+1) shape?\n",
    "assert polynomial_expand(x_train, 10)[0, 0] == 1  # column of 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a1a19daed76363f0a5eb480e19b287e",
     "grade": false,
     "grade_id": "cell-dd2ae7f7fd750a51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 5** (open question): Solve the following questions using the functions that you have written above. \n",
    "\n",
    "**5.a. (1pt)** Perform a polynomial regression to fit `x_train, y_train` and obtain `theta` with $k = 1 \\dots 12$ and then predict labels for the test set `x_test`. \n",
    "\n",
    "The test error is defined as previously as\n",
    "$$ E_\\mathrm{test} (\\theta ; \\hat{\\boldsymbol{y}}_\\mathrm{test}, \\boldsymbol{y}_\\mathrm{test}) =  \\|\\hat{\\boldsymbol{y}}_\\mathrm{test} - \\boldsymbol{y}_\\mathrm{test}\\|_2^2, $$\n",
    "where $\\boldsymbol{y}_\\mathrm{test}$ is the vector of all true values we're trying to predict from all the samples in `x_test` and $\\hat{\\boldsymbol{y}}_\\mathrm{test}$ is the vector of values that we're predicting given the vector $\\theta$ learned using `x_train, y_train`. The training error has the same expression, using the training set, i.e. $ E_\\mathrm{train} (\\theta ; \\hat{\\boldsymbol{y}}_\\mathrm{train}, \\boldsymbol{y}_\\mathrm{train}) =  \\|\\hat{\\boldsymbol{y}}_\\mathrm{train} - \\boldsymbol{y}_\\mathrm{train}\\|_2^2$.\n",
    "\n",
    "Store the training errors and test errors you obtain in two lists of size $12$, which names must be (respectively) `train_errors` and `test_errors`, and such that `train_errors[k]` (resp. `test_errors[k]`) gives the training error (resp. test error) obtained by performing polynomial regression with maximal degree $k$.\n",
    "\n",
    "*Remark:* you are **not** allowed to use `sklearn`.\n",
    "\n",
    "*Hint:* Notice you already implemented the cost function, the polynomial expansion and the linear regression, but pay attention to the size of their inputs / outputs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9acaace6fffbaff59cfc235ac21a3352",
     "grade": false,
     "grade_id": "cell-81ce4014cf2ed71d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# train_errors = ...  # <- TO UNCOMMENT AND COMPLETE\n",
    "# test_errors = ...  # <- TO UNCOMMENT AND COMPLETE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d055fe687946639daa8ff09227338f3",
     "grade": true,
     "grade_id": "cell-be3100266c07d9c6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(train_errors) == 12  # Training errors is a list of size 12?\n",
    "assert len(test_errors) == 12  # Test errors is a list of size 12?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bf3fd553dcc8d672a4a0a6d0b6a7ad6",
     "grade": false,
     "grade_id": "cell-2b2cbc7b9a5f9841",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**5.b. (0.5pt)** Plot both arrays, i.e. the training and test errors, w.r.t. the polynomial degree $k$ on the same graph in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84e124f779f036acc976530d38153255",
     "grade": true,
     "grade_id": "cell-71d21dd090d1888f",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "173d23aa37bbb8752294ebb693d6b4c3",
     "grade": false,
     "grade_id": "cell-2d086efe633593d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**5.c. (0.5pt)** What is the name of the phenomenon / phenomena we are observing here? (short qualitative answer in subsequent cell - bonus points for conciseness AND completeness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "989156a941ae35c221a91abae05e6b24",
     "grade": true,
     "grade_id": "cell-13b2842f627eba34",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f582eac5172e9569dba6e8cb47f36e08",
     "grade": false,
     "grade_id": "cell-8c2be0654fb9f25e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 2: logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e371fd2c266b280e7bb4695f239fe5c",
     "grade": false,
     "grade_id": "cell-ada4c9868a016529",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The logistic regression is a common model used to perform binary classification that is actually formulated as a regression problem. \n",
    "\n",
    "We have a *learning / training set* $\\mathcal{D} = \\{\\boldsymbol{x^{(i)}}, y^{(i)}\\}_{1 \\leq i \\leq n}$, where $y_i \\in \\{0,1\\}$ and $x_i \\in \\mathbb{R}^p$.\n",
    "\n",
    "We consider the logistic / sigmoid function:\n",
    "$$ \\sigma : t \\mapsto \\frac{1}{1 + e^{-t}}.$$\n",
    "Note that $\\sigma$ takes values in $(0,1)$ and will be used to estimate $y^{(i)}$ given $x^{(i)}$. More precisely, given a weight vector $\\theta = ( \\theta_0, \\dots, \\theta_p )^T$ and a vector $x^{(i)} = (1, x^{(i)}_1, \\dots ,x^{(i)}_p)^T$, we make the following estimation:\n",
    "$$\n",
    "    \\hat{y}^{(i)} = \\begin{cases} 1 \\text{ if } \\sigma(x^{(i)T} \\theta) > 1/2 \\\\\n",
    "                                  0 \\text{ otherwise } \n",
    "                    \\end{cases}\n",
    "$$\n",
    "\n",
    "We want to find a good $\\theta$. One can show that the optimal (in the sense of the *likelihood*) $\\boldsymbol{\\theta}^* = \\begin{pmatrix} \\theta_0^* & \\dots & \\theta_p^* \\end{pmatrix}^T$ minimizes the following optimization problem:\n",
    "\n",
    "$$\\boldsymbol{\\theta}^* \\leftarrow {\\arg\\!\\min}_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta}) \\quad \\text{with} \\quad E(\\boldsymbol{\\theta}) = - \\sum_{i=1}^n (1 − y^{(i)}) \\ln(1 − \\sigma^{(i)}) + (y^{(i)}) \\ln(\\sigma^{(i)})$$\n",
    "and\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\begin{pmatrix} 1 & x_1^{(1)} & \\dots & x_p^{(1)} \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & x_1^{(n)} & \\dots & x_p^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{y} = \\begin{pmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{\\theta} = \\begin{pmatrix} \\theta_0 \\\\ \\vdots \\\\ \\theta_p \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0618452021b7d23aceed4c4664983112",
     "grade": false,
     "grade_id": "cell-0ebbf5944e3efc3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 1: (0.5pt)** Does the error function $E$ have a unique minimum? Why? If it exists, is there an analytical (closed form) solution corresponding to this minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33d32a06ab6a14db1eb99c912aaedaad",
     "grade": true,
     "grade_id": "cell-58a7eeda9a347246",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ea3aa1ae29642abba87471452378f38",
     "grade": false,
     "grade_id": "cell-1219083b8473dc2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Iris dataset\n",
    "We now consider the `iris` dataset of $n = 150$ points, with $3$ labels 'setosa', 'versicolor', and 'virginica', encoded respectively by $0, 1, 2$ in the following. This dataset is split in a training set `x_train, y_train` with $3/4$th of the $150$ observations, and a test set `x_test, y_test` of $1/4$th of the $150$ observations; this dataset must be loaded by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf84fa34b690ba7ed415e28afa63a52b",
     "grade": false,
     "grade_id": "cell-6d44d9e2963bc560",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2][np.where([target in [0, 1] for target in iris.target])[0]]\n",
    "y = iris.target[np.where([target in [0, 1] for target in iris.target])[0]]\n",
    "np.random.seed(seed=1)\n",
    "x_train, x_test, y_train, y_test = sk.model_selection.train_test_split(X, \n",
    "                                                                       y, \n",
    "                                                                       test_size=1/4, \n",
    "                                                                       random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1aa694529e900a1ca2f2d8b8a73d7417",
     "grade": false,
     "grade_id": "cell-e1ce6683b988c61c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The description of this dataset (not mandatory for what follows - note that we've kept only the two first features, sepal length and sepal width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afbb67753fb27a4428677aa3c0b113be",
     "grade": false,
     "grade_id": "cell-7b8caf761f2c4f8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12619ef8025b895fad28ade0e07125aa",
     "grade": false,
     "grade_id": "cell-f56d2f05b939032c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "10 first rows of `x_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35f4d8789c2a20fbc1ad657abcf5a42d",
     "grade": false,
     "grade_id": "cell-9b3b50d2de445586",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd3818963d743aed58c7d0d286460ee8",
     "grade": false,
     "grade_id": "cell-d31bb0b9678d8961",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The corresponding labels (corresponding resp. to 'setosa', 'versicolor', and 'virginica')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f34edf8ff8b6e32596b9f847f705e1df",
     "grade": false,
     "grade_id": "cell-8587879ab1376037",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_train[:10].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e663f3bc5c5430c00bdaadf2449813af",
     "grade": false,
     "grade_id": "cell-f107e704e59664aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2:** \n",
    "\n",
    "**2.a. (1pt)** Implement a function `sigma` which computes, given $x \\in \\mathbb{R}^p$ and $\\theta \\in \\mathbb{R}^{p+1}$ the quantity $\\sigma(x^{T} \\theta)$.\n",
    "\n",
    "_Python hints_ : \n",
    "- You can use `np.exp(...)` to compute the exponential. \n",
    "- You can use `np.concatenate((A, B))` to concatenate two numpy arrays. \n",
    "- You can use `np.dot(A,B)`, or equivalently `A.dot(B)` to compute the matrix-matrix (or matrix-vector) $A \\cdot B$ product between two numpy arrays `A` and `B`. Equivalently, you can use matrix multiplication with vectors (either `A @ B` or `np.matmul(A, B)`), but beware of the shapes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb6c4e2f9a2e8e815b95126d46518e86",
     "grade": false,
     "grade_id": "cell-360b8a9f9267273f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigma(x: np.array, theta: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function\n",
    "\n",
    "    :param numpy.array x: input vector shape p\n",
    "    :param numpy.array theta: parameter vector of shape (p + 1)\n",
    "    :return: probability for class 1\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7b0a693e1e6b303daf7449288c9a085",
     "grade": true,
     "grade_id": "cell-7108af18cc72c7ea",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert sigma(X[0,:], np.array([0, 0, 0])) == 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7a3d3b5e7df9ed7164e92e25229653c",
     "grade": false,
     "grade_id": "cell-d29e03b8313bb079",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.b. (1pt)** Implement a function `pred` which provides, given $x \\in \\mathbb{R}^p$ and $\\theta \\in \\mathbb{R}^{p+1}$, an estimate $\\hat{y}$ of $y$ as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3b6800cfa2bab18e4f2cef3c4594847",
     "grade": false,
     "grade_id": "cell-773a82c0b8c37472",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pred(x: np.array, theta: np.array) -> int:\n",
    "    \"\"\"\n",
    "    Compute a class prediction from a class probability\n",
    "\n",
    "    :param numpy.array x: input vector of shape p\n",
    "    :param numpy.array theta: parameter vector of shape p + 1\n",
    "    :return: predicted class\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "442c6951b5dc0f7a1bb9129811327c67",
     "grade": true,
     "grade_id": "cell-959c2fc7d7bcce47",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert pred(X[0,:], np.array([1,1,1])) == 1  # Test for a particular value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d15ae9dbc60d1b69e4c7016a528ea6df",
     "grade": false,
     "grade_id": "cell-55c86ffda2f75cf1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 3 (1pt):** Implement the error function $E$ defined above, and which we want to subsequently minimize w.r.t. $\\theta$. It takes $X, y, \\theta$ as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40436add838dfed27d201abfcb932ece",
     "grade": false,
     "grade_id": "cell-fbf29274424a6814",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def E(X: np.array, y: np.array, theta: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Compute error for logistic regression\n",
    "\n",
    "    :param numpy.array X: design matrix of shape N x p\n",
    "    :param numpy.array y: vector of real classes of shape N\n",
    "    :param numpy.array theta: parameter vector of shape (p+1)\n",
    "    :return: error\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffa6a1478b7186cafa408c13226a721f",
     "grade": true,
     "grade_id": "cell-8b1e948e13207a66",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(E(X, y, np.array([0, 0, 0])))  # Prints a particular value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bfc572489be9a021cb88c96f71e8f83",
     "grade": false,
     "grade_id": "cell-bbe400d9a297c667",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As there is no closed form for the optimal $\\theta^*$, we will minimize $E$ using a gradient descent. Compute, either here or on a separate sheet of paper (upload it in a **new** Markdown cell via Edit > Insert Image), the **analytical expression** (as opposed to a particular value given some particular inputs $\\boldsymbol{X}, \\boldsymbol{y}$) gradient of $E$ w.r.t. $\\boldsymbol{X}, \\boldsymbol{y}, \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71ce3f1305be248b8f098a8a764f5159",
     "grade": true,
     "grade_id": "cell-21adfd64205ff93c",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65db5670f63f799f5be5e604419b914b",
     "grade": false,
     "grade_id": "cell-6ed711065b055274",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement a function `grad_E(X,y,theta)` which returns the gradient (with respect to $\\theta$) of $E$ for a given estimate $\\theta$. Again, beware of the shape of the input arguments. You can make use of the `sigma` function already implemented, the `np.concatenate`, `np.dot`, `np.matmul`, `@` functions already suggested earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d04a828f5f86f7041ace80ce127f5e1",
     "grade": false,
     "grade_id": "cell-e097edeec88dd6ba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def grad_E(X: np.array, y: np.array, theta: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Compute the gradient of the error w.r.t. theta\n",
    "\n",
    "    :param numpy.array X: design matrix of shape n x p\n",
    "    :param numpy.array y: vector of real classes of shape n\n",
    "    :param numpy.array theta: parameter vector of shape (p+1)\n",
    "    :return: gradient of shape (p+1)\n",
    "    :rtype: numpy.array\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfd1f572fa3132690133c074318fdf15",
     "grade": true,
     "grade_id": "cell-20bd5c4608562daa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(grad_E(X, y, np.array([0,0,0]))) == 3  # Shape of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b08a139adf24c7abc30cffe6c8e5f93",
     "grade": false,
     "grade_id": "cell-dec3055272cbd6a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 4 (1pt):** Complete the following code in order to implement the gradient descent algorithm to minimize $E$ which returns the final estimation of $\\theta$ and the evolution of energy over time. The hyper parameters (number of steps, learning rates, stopping criterion) are set by default and should not be changed. Note that you **must** implement an early stopping criterion. Recall the complete algorithm:\n",
    "\n",
    "Starting from a random point $\\boldsymbol{\\theta}$, the gradient descent method proposes a new point \n",
    "$\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta \\nabla_{\\boldsymbol{\\theta}}E(\\boldsymbol{\\theta})$ at each iteration $t$ until a stopping criterion has been reached: e.g. $E^{(t-1)}(\\boldsymbol{\\theta}^{(t-1)}) - E^{(t)}(\\boldsymbol{\\theta}^{(t)}) < \\epsilon$ with $\\epsilon$ a chosen minimal improvement of the error, $t$ and $t-1$ denoting the current (resp. previous) estimation of the error $E$ and the parameter $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb63dd819306c35f18b67e1d90ba50df",
     "grade": false,
     "grade_id": "cell-00cce9f597201ea5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def grad_descent(X: np.array, y: np.array, eta: float = 0.001,\n",
    "                 nb_max_step: int = 1000, stopping_criterion: float = 0.01,\n",
    "                 seed: int = 1):\n",
    "    \"\"\"\n",
    "    Perform gradient descent\n",
    "    Remark: do not change the default values, do not normalize the gradient by the number of points!\n",
    "\n",
    "    :param numpy.array x: input observations, shape N x p\n",
    "    :param numpy.array y: input labels, shape N (filled with 0 and 1)\n",
    "    :param float eta: learning rate.\n",
    "    :param int nb_max_step: maximal number of steps done in the gradient descent\n",
    "    :param float stopping_criterion: stopping criterion on gradient process\n",
    "    :param int seed: numpy random seed\n",
    "    :return: last parameter value of shape p+1 (theta), list of float (evolution_of_loss)\n",
    "    :rtype: numpy.array, list\n",
    "    \"\"\"\n",
    "    # Storing some useful values\n",
    "    N, p = X.shape\n",
    "    # List to store the evolution of E(theta) over steps\n",
    "    evolution_of_loss = []\n",
    "    # Store the current value of loss to stop gradient descent\n",
    "    e = np.inf\n",
    "    # Initialization of theta\n",
    "    np.random.seed(seed=seed)  # DO NOT change this\n",
    "    theta = np.random.rand(p + 1)  # DO NOT change this\n",
    "\n",
    "    ### Perform the gradient descent\n",
    "    for t in range(nb_max_step):\n",
    "        \n",
    "        # Perform a gradient step\n",
    "        # grad = ... # <-- to complete\n",
    "        # theta = ... # <-- to complete\n",
    "        # new_e = ... # <-- to complete, store the loss according to the new parameter theta\n",
    "        # Do not normalize by the number of points!\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    return theta, evolution_of_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1399fcbdae7d65f5cd1062aeaa6ee703",
     "grade": false,
     "grade_id": "cell-a7aef4bf892a366a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your code with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc9a4a3e0b4aba16e7e4f6a7a151d02e",
     "grade": true,
     "grade_id": "cell-d5b379f387a36410",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "theta, evol = grad_descent(x_train, y_train)  # Perform gradient descent\n",
    "plt.plot(evol);  # Plot evolution of loss: does this seem correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7fc36d9a3e9306afddf1bfe1413e7156",
     "grade": false,
     "grade_id": "cell-4c8243c513681f75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 6:** \n",
    "\n",
    "**6.a. (0.5pt)** What is the training error rate, defined in classification as the percentage of predicted labels different from the true ones, of the logistic regression on the training dataset? Put the result in `training_error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6000749a7b60d23b2f7c799e43ea12c6",
     "grade": false,
     "grade_id": "cell-6b39a9e76dabfea2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# training_error = ...  # <- TO UNCOMMENT AND COMPLETE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42bbae961dc9eb59fea8c67623f80893",
     "grade": true,
     "grade_id": "cell-a3b9d7376a463f52",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 0 <= training_error <= 1  # Must be between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0dbdcc7bb6f2a419e8e8cd972e403553",
     "grade": false,
     "grade_id": "cell-310bbb56f4171164",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**6.b. (0.5pt)** What is the test error rate, defined in classification as the percentage of predicted labels different from the true ones, of the logistic regression on the test dataset? Put the result in `test_error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "073ec5cc3d0bcd1006c607be8eb943ee",
     "grade": false,
     "grade_id": "cell-396ca15afc849d3d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test_error = ...  # <- TO UNCOMMENT AND COMPLETE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3be549026ba1fd7bf2d8be2bccb994e6",
     "grade": true,
     "grade_id": "cell-fdf1c6263fed6ef6",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 0 <= test_error <= 1  # Must be between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8332c545ad16c6a59d9181ab4d9d3df2",
     "grade": false,
     "grade_id": "cell-ae4438a57031068f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 3: $k$-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f04b74bfb63858fb1344c455a9fbb94d",
     "grade": false,
     "grade_id": "cell-204d8e96ddde0535",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We briefly recall the $k$-nearest neighbors ($k$-NN) algorithm seen in lab session 2.\n",
    "\n",
    "Consider a dataset $\\mathcal{D} = (X, y)$ where the $j$-th coordinate of the $i$-th observation is denoted by $x^{(i)}_j$ and the corresponding label is $y^{(i)}$, or equivalently:\n",
    "$$\\boldsymbol{X} = \\begin{pmatrix} x_1^{(1)} & \\dots & x_p^{(1)} \\\\ \\vdots & \\dots & \\vdots \\\\ x_1^{(n)} & \\dots & x_p^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{y} = \\begin{pmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{pmatrix}.$$\n",
    "\n",
    "Fix an integer $k$. Given a new observation $x \\in \\mathbb{R}^p$, we predict its class $\\hat{y}$ in the following way.\n",
    "\n",
    "Let $x^{(i_1)}, \\dots, x^{(i_k)}$ be the $k$ observations in $X$ which minimize the Euclidean distance $\\|x - x^{(i)}\\|_2$.\n",
    "\n",
    "In classification, we choose $\\hat{y}$ as the dominant occurence among the corresponding classes $\\{ y^{(i_1)}, \\dots, y^{(i_k)} \\}$ (majority vote). In case of a tie, i.e., two (or more) classes are the most frequent, $\\hat{y}$ is chosen randomly (uniformly) among these equally dominant classes.\n",
    "\n",
    "In regression, we compute $\\hat{y}$ as the mean of the values of $\\{ y^{(i_1)}, \\dots, y^{(i_k)} \\}$.\n",
    "\n",
    "We generate some data so that you can test your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d277af93795661090f8a605cd8d7ea67",
     "grade": false,
     "grade_id": "cell-540227a0f5278f23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "knn_df = gen_d_regression_samples(n_samples=50, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d4e7f879015208bead595eb05332541",
     "grade": false,
     "grade_id": "cell-aa5c33c0ad7f407c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "knn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01854171d117a75b2c629f13b8cb34dc",
     "grade": false,
     "grade_id": "cell-334b4a80a13a0e5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## $1$-nearest neighbors regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12d87d94aec6ffb7229fde137a6c4b2a",
     "grade": false,
     "grade_id": "cell-3b3a4cc001ada780",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 1 (1pt):** Implement a function which takes as input a matrix `X_train` of shape `n x p`, corresponding values `y_train` (shape `n`) a new observation `x` of shape `p`, and which returns a prediction of `y_hat` for this new observation using the **one**-nearest neighbor algorithm.\n",
    "\n",
    "_Python hint:_ You can compute the norm of a numpy array using `np.linalg.norm`. You can use `np.argmin` (resp. `np.argmax`) to find the index of the minimum (resp. maximum) of a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "462a7790ae149d7f74773f0d9cbbe99c",
     "grade": false,
     "grade_id": "cell-986f4953a4bfaba3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def oneNN(X_train: np.array, y_train: np.array, x: np.array) -> int:\n",
    "    \"\"\"\n",
    "    Search nearest neighbor of x in X_train and output its predicted label\n",
    "\n",
    "    :param numpy.array X_train: training dataset of shape N x p\n",
    "    :param numpy.array y_train: training labels of shape N\n",
    "    :param numpy.array x: test point of shape p\n",
    "    :return: y_hat, class prediction\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    # First, we construct a list of distances from x to each data point in X_train.\n",
    "    # Each distance is computed using np.linalg.norm(x - x_train).\n",
    "    # Then, we predict y_hat label by choosing the label of the closest point in X_train (1-NN).\n",
    "    # The smallest distance is the min in the list. However, we don't need the distance itself but its position.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49e8cac957d0a03e87b5cbd5b2c044dc",
     "grade": false,
     "grade_id": "cell-fe3436b3675e8d43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your function by running the following cell (note that `df` is a DataFrame and we require a numpy array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31b9c4dd4fecc33072c56b83a4276edf",
     "grade": true,
     "grade_id": "cell-b58b1d0e0fd68292",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the value y_hat corresponding to the closest point in knn_df to the origin\n",
    "print(oneNN(knn_df[['x0', 'x1']].to_numpy(),\n",
    "            knn_df['y'].to_numpy(), np.array((0,0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a882ca5ce226454cb8fba750b51ed8ae",
     "grade": false,
     "grade_id": "cell-a58f5427c4b895a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2 (1pt):** What is the training error (recall that in classification, it is the proportion of misclassified points in the training dataset; in regression, the squared sum of errors) achieved by this **algorithm** for $k=1$?\n",
    "\n",
    "*Hint*: you can try returning `y_hat` from your previous function for `x in X_train` and compare it to `y_train` to give you an idea in the following cell (optional), and explain your answer in the subsequent cell (mandatory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "650bcb06cae3031abb571cf1e9eb00aa",
     "grade": true,
     "grade_id": "cell-97c20bb2c7fc3f7d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ae37d0fde80b5d0fdb34cb50db167fd",
     "grade": true,
     "grade_id": "cell-5221963547af5022",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d376689dba6a8bab51b62bfa262d873",
     "grade": false,
     "grade_id": "cell-133fbd8fd1e06cf1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## $k$-nearest neighbors classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a73e13793b4695ffc72d10f86b6f628",
     "grade": false,
     "grade_id": "cell-c5c9cb47ac424bf3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 3 (1pt):** Is the $k$-NN algorithm a _parametric_ or _non-parametric_ model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44b5434eeac5cda6f5937d9acd89051a",
     "grade": true,
     "grade_id": "cell-62c8d71883d9e033",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c1aadb11ab106f836af3a362b6281b9",
     "grade": false,
     "grade_id": "cell-5dcf4593554f299a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We go back to the iris dataset (in particular, its two first features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62e93c488b3c8cb1b9f49877be68dd7e",
     "grade": false,
     "grade_id": "cell-53346579b64b17e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = sk.model_selection.train_test_split(iris.data[:, :2], \n",
    "                                                                       iris.target, \n",
    "                                                                       test_size=1/4, \n",
    "                                                                       random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70f2f26f380d74c7ee328bfd5bd09b58",
     "grade": false,
     "grade_id": "cell-c85ab85abd4b2b95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 4 (1pt):** In this question, we allow the use of the `scikit-learn` library. \n",
    "\n",
    "**4.a.** Using the library, perform $k$-NN classification for all $k = 1 \\dots 20$ by training on (`x_train`, `y_train`), and use it to predict labels for `x_train` and `x_test`. Evaluate the quality of the classification using the `accuracy_score` function provided by `scikit-learn`. Store the results in two lists, `accuracy_scores_train` and `accuracy_scores_test`, which should contain, for $k = 1 \\dots 20$, the error rate (evaluated by `accuracy_score`) on `x_train` and `x_test` respectively.\n",
    "\n",
    "We recall how to perform $k$-NN classification using `scikit-learn`:\n",
    "- `clf = KNeighborsClassifier(n_neighbors=k)` instantiates an object `clf` of the class `classifier`, which can be used to perform `n_neighbors`-NN classification (of course, you have to provide the value of `k` when instantiating the classifier).\n",
    "- `clf.fit(x_train, y_train)` trains this classifier on a training set, where `x_train` is the observation matrix, `y_train` contains the labels.\n",
    "- `y_pred = clf.predict(x_test)` for a new set of observations, encoded as a matrix `x_test`, returns label predictions `y_pred`.\n",
    "- `accuracy_score(y_true, y_pred)` given the true labels of the test set `y_true` (`y_test` here) and the labels `y_pred` predicted by the $k$-NN classifier, computes the accuracy score of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81dc8ad5d3e17125a77b7b54db5c4490",
     "grade": false,
     "grade_id": "cell-e8faa1133f8e052d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# accuracy_scores_train = ...  # <- TO UNCOMMENT AND COMPLETE\n",
    "# accuracy_scores_test = ...  # <- TO UNCOMMENT AND COMPLETE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cd45573e566fc9abe85a3efa34ec685",
     "grade": true,
     "grade_id": "cell-61680ac47d77b409",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(accuracy_scores_train) == 20  # Did you compute 20 values of k?\n",
    "assert np.max(accuracy_scores_train) <= 1  # The proportion should be between 0 and 1\n",
    "assert np.min(accuracy_scores_train) >= 0  # The proportion should be between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72a47284bae359487a53080155a9d1dc",
     "grade": false,
     "grade_id": "cell-24e5e75155718207",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can plot the accuracies on the same graph w.r.t. to $k$ using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "640d0ce8905a6de703d1e70c89ae30a9",
     "grade": false,
     "grade_id": "cell-aef72ca1804ff6ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,21), accuracy_scores_train, label='train')\n",
    "plt.plot(range(1,21), accuracy_scores_test, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "134facec219cb31f42fca94fa97f84ff",
     "grade": false,
     "grade_id": "cell-d2c994d500c1b145",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**4.b. (0.5pt)** Which value(s) of $k$ yield(s) the best accuracy on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b55c6224e7f133d664f8d321f7755c36",
     "grade": true,
     "grade_id": "cell-6cc02b291acbc71b",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d185e3ad40bf35a41394ef763bc627ae",
     "grade": false,
     "grade_id": "cell-4daca7c0cc3bf563",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**4.c. (0.5pt)** What is the phenomenon you are witnessing for $k > 6$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b655d85b946c25a5ef429b41d2b9966",
     "grade": true,
     "grade_id": "cell-9d8dcadd1da7a9b5",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a2c8542b363b68333e8b331f93773df",
     "grade": false,
     "grade_id": "cell-1f74ceb26ddb75f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**4.d. (0.5pt)** What is the phenomenon you are witnessing for $k < 6$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "26e0cab160da269483ad30a440594658",
     "grade": true,
     "grade_id": "cell-aae5a54bb86da485",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d26d28d128b33de040ecbb86407f8078",
     "grade": false,
     "grade_id": "cell-2c3a79955aead2f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Additional exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83648ef5fc5697bc502f2681370ff92a",
     "grade": false,
     "grade_id": "cell-e741d13053466d2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35488576d1abb2591edf62028f5b978c",
     "grade": false,
     "grade_id": "cell-39944dd763705a9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Open question (2pts)** Perform cross-validation of $k$ for the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ec417bf02f61e3cd9cc5f5d9a30c5ef",
     "grade": true,
     "grade_id": "cell-83ffb38d57b3219b",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "titanic = pd.read_csv(\"https://raw.githubusercontent.com/adimajo/CSE204-2021/master/data/titanic_train.csv\")\n",
    "titanic = titanic.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], axis=1)\n",
    "titanic.dropna(inplace=True)\n",
    "titanic['Embarked'] = titanic['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n",
    "titanic['Sex'] = titanic['Sex'].map({'male': 0, 'female': 1}).astype(int)\n",
    "X = titanic.drop(\"Survived\", axis=1).to_numpy()\n",
    "y = titanic[\"Survived\"].to_numpy()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6586355b89381ad66edc04e8905b1300",
     "grade": false,
     "grade_id": "cell-2f206408dd0a08a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Additional exercise 2 (hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e443c1b08f8ce35a2b324346c4cf5b07",
     "grade": false,
     "grade_id": "cell-304134671e25a59b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Ridge Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fc833cd7000c1ce1ee85ad69ba4de6e",
     "grade": false,
     "grade_id": "cell-177cf68ee2e695e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Open question (2pts)** Add the ridge penalty to the gradient descent algorithm implemented for logistic regression, apply it to the Titanic dataset, and perform cross-validation to choose the amount of penalty. We can expect the code be computationally expensive, so a sketch of the solution is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cff19a15b1a9185e5550e4555133855a",
     "grade": true,
     "grade_id": "cell-185782481ccb9b8d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def grad_descent(X: np.array, y: np.array, eta: float = 0.0001,\n",
    "                 nb_max_step: int = 1000, stopping_criterion: float = 0.00001,\n",
    "                 alpha: float = 0.01) -> np.array:\n",
    "    \"\"\"\n",
    "    Compute gradient descent with ridge penalty for logistic regression\n",
    "\n",
    "    :param numpy.array X: design matrix of shape (N, p)\n",
    "    :param numpy.array y: vector of responses of shape N\n",
    "    :param float eta: step size\n",
    "    :param int nb_max_step: maximum number of iterations\n",
    "    :param float stopping criterion: whenever theta does not move more than this criterion, stop\n",
    "    :return: last value of parameter, list of loss values, list of values of theta\n",
    "    :rtype: numpy.array, list, list\n",
    "    \"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc302116f62c00460400e92f7dc9a3fb",
     "grade": true,
     "grade_id": "cell-9ed8a7af614ec81c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5992964ddf8a887642bf5482d9f044d",
     "grade": true,
     "grade_id": "cell-b8395e9a12208743",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "166b0a6df502f6ecc82f385832d283e2",
     "grade": true,
     "grade_id": "cell-9541c911520c15e6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f33ac9ab3a658636e82ee168d32f221a",
     "grade": true,
     "grade_id": "cell-4cb7446ca8169ae5",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSE204",
   "language": "python",
   "name": "cse204"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
