{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccec0d1fc6a1f2c8a0244c7ed87cf4d0",
     "grade": false,
     "grade_id": "cell-a25c0c792fa938b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CSE 204 Lab 14: Kernel methods\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/adimajo/CSE204-2021/master/data/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSE204-2021](https://moodle.polytechnique.fr/course/view.php?id=12838) Lab session #14\n",
    "\n",
    "Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95cb605e4c49de6c329b4b420d31cd34",
     "grade": false,
     "grade_id": "cell-9ed05ba8846d12be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you will implement two kernel methods: Kernel k-Means and Kernel PCA.\n",
    "While libraries such as scikit-learn implement these algorithms, they are simple enough for you to implement with numpy alone.\n",
    "Before starting, import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "870b22f43815431bb9f7ed138449f7e8",
     "grade": false,
     "grade_id": "cell-ad810c8a728ffb57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2644caeca2b079c0afbaef05b24ff900",
     "grade": false,
     "grade_id": "cell-2c2ed2be86e4374f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1: Kernel methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b93fa2b91de878676e5cfc0eaa1b8c2",
     "grade": false,
     "grade_id": "cell-694a2134d32f649e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "### Introduction\n",
    "\n",
    "Some machine learning algorithms we have seen so far only work for \"linear problems\".\n",
    "\n",
    "For instance:\n",
    "- with linear regression ([`lab_session_02`](https://adimajo.github.io/CSE204-2021/lab_session_02/lab_session_02.html)) we fit a hyperplane to predict unknown values, but the method can't effectively predict $f(\\boldsymbol{x})$ if the (unknown) $f$ function is non-linear with respect to $\\boldsymbol{x}$;\n",
    "- with linear classification methods, e.g. logistic regression ([`lab_session_05`](https://adimajo.github.io/CSE204-2021/lab_session_05/lab_session_05.html)), the decision boundary is a hyperplane;\n",
    "- the Principal Component Analysis (PCA) algorithm for feature extraction ([`lab_session_10`](https://adimajo.github.io/CSE204-2021/lab_session_10/lab_session_10.html)) can only find \"linear directions\" (meaning linear combinations of the original features) in the data;\n",
    "- in the k-means algorithm presented last week for clustering tasks ([`lab_session_12`](https://adimajo.github.io/CSE204-2021/lab_session_12/lab_session_12.html)), the separating boundary between clusters is linear (i.e. k-Means can only detect convex clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc79165e2e46b05fb076412e368dad62",
     "grade": false,
     "grade_id": "cell-32de0bf38a060c5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As we have seen in [`lab_session_04`](https://adimajo.github.io/CSE204-2021/lab_session_04/lab_session_04.html), where we introduced polynomial regression, we can extend all these methods to \"nonlinear problems\" by applying a feature mapping $\\phi$ on the data.\n",
    "This function maps input data $\\boldsymbol{x}_i \\in \\mathbb{R}^d$ into another space (the *feature space*) $\\phi(\\boldsymbol{x}_i) \\in \\mathbb{R}^{\\hat{d}}$ where the \"linear algorithm\" is actually effective.\n",
    "Here we denote by $\\mathcal{X}$ the input space and by $\\mathcal{H}$ the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f2e0366266f5a9a8e183126275e71ffd",
     "grade": false,
     "grade_id": "cell-2f113d9ea580dfb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To be effective, the feature space is defined as a very high dimension space in many practical problems (usually $\\hat{d} \\gg d$, contrary to what we did in [`lab_session_10`](https://adimajo.github.io/CSE204-2021/lab_session_10/lab_session_10.html) and [`lab_session_11`](https://adimajo.github.io/CSE204-2021/lab_session_11/lab_session_11.html) with PCA and autoencoders respectively, where the objectives were visualization and compression - $d$ was already \"big\" - no pun intended).\n",
    "Thus often, projections and computations in such a space are tedious and require a lot of computational power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b3440ec7c7545dfd1e4bd7cd664d04d",
     "grade": false,
     "grade_id": "cell-8984b069f136e946",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "So here comes the *Kernel trick*.\n",
    "The basic idea is to project data $\\boldsymbol{x}_i$ in some high dimension feature space $\\mathcal{H}$ and apply an adapted version of the \"linear algorithms\" in this space without explicitly computing the projection $\\phi(\\boldsymbol{x}_i)$.\n",
    "\n",
    "Using the kernel trick, we often don't even know what the feature map $\\phi$ actually is!\n",
    "Instead, computations are made implicitly from $\\mathcal{X}$ through a carefully chosen function $K: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ which computes some kind of \"similarity\" between two points of $\\mathcal{X}$.\n",
    "\n",
    "This similarity function $K$ named *kernel* is chosen such that it represents a dot product in the high-dimensional feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34025573c358a6f4f36f43777543128d",
     "grade": false,
     "grade_id": "cell-77a2d22e8609dce7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Thus, to be a valid kernel, $K$ have to be defined such that there is a $\\phi$ function for which the following holds: $K(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\phi(\\boldsymbol{x}_i)^{\\top} \\phi(\\boldsymbol{x}_j)$ for all $\\boldsymbol{x}_i, \\boldsymbol{x}_j \\in \\mathcal{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85ebad911f84aaa86aef0cfdaf6f9218",
     "grade": false,
     "grade_id": "cell-f12c720bd54ccb38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The *Kernel matrix* $\\mathbf{K}$ is a $n \\times n$ matrix containing the pairwise similarity values between points in the dataset $\\mathbf{D} = (\\boldsymbol{x}_i)_1^n \\subset \\mathcal{X}$:\n",
    "$$\n",
    "\\mathbf{K} =\n",
    "\\begin{pmatrix}\n",
    "K(\\boldsymbol{x}_1, \\boldsymbol{x}_1) & K(\\boldsymbol{x}_1, \\boldsymbol{x}_2) & \\cdots & K(\\boldsymbol{x}_1, \\boldsymbol{x}_n)\\\\\n",
    "K(\\boldsymbol{x}_2, \\boldsymbol{x}_1) & K(\\boldsymbol{x}_2, \\boldsymbol{x}_2) & \\cdots & K(\\boldsymbol{x}_2, \\boldsymbol{x}_n)\\\\\n",
    "\\vdots                        & \\vdots                        & \\ddots & \\vdots\\\\\n",
    "K(\\boldsymbol{x}_n, \\boldsymbol{x}_1) & K(\\boldsymbol{x}_n, \\boldsymbol{x}_2) & \\cdots & K(\\boldsymbol{x}_n, \\boldsymbol{x}_n)\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bb72e615f7e122455d87f7b44f7bda8",
     "grade": false,
     "grade_id": "cell-7d97045d228ca7bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Eventually, the kernel function $K$ allows us to compute a dot product in $\\mathcal{H}$ without explicitly knowing (and calculating) $\\phi(\\boldsymbol{x})$. The idea is then to find some machine learning algorithms that can be rewritten such that they only require the dot product $\\phi(\\boldsymbol{x}_i)^{\\top} \\phi(\\boldsymbol{x}_j)$ in the feature space, *i.e.*, algorithms where $\\phi$ only appears in the form of a dot product $\\phi(\\boldsymbol{x}_i)^{\\top} \\phi(\\boldsymbol{x}_j)$ (so that eventually all computations in $\\mathcal{H}$ can be performed exclusively using $K$).\n",
    "\n",
    "PCA, k-Means and ridge regression are such compatible (*kernelizable*) algorithms: that is approximately what we did for spectral clustering [in the last lab](https://adimajo.github.io/CSE204-2021/lab_session_13/lab_session_13.html) where the kernel was given by the $k$ nearest neighbors.\n",
    "\n",
    "$||\\cdot||$ will denote the L2 norm in what follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71744c52e84ab32e681f52ed4788fb46",
     "grade": false,
     "grade_id": "cell-cfbe4767237b6990",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will now present some classical kernels. Let's start with the simplest one: the linear kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fda91d336a9944e1eafe451cbc131387",
     "grade": false,
     "grade_id": "cell-840c9c6d2e4a8f4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### The linear kernel\n",
    "\n",
    "The linear kernel is defined as: $K(\\boldsymbol{x}, \\boldsymbol{y}) = \\boldsymbol{x}^{\\top} \\boldsymbol{y}$ <br>\n",
    "with $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathcal{X}$.\n",
    "\n",
    "Thus the feature mapping associated to this kernel is the identity mapping $\\phi(\\boldsymbol{x}) \\mapsto \\boldsymbol{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ff146fe6e9f074e43100ed75afbbfeb",
     "grade": false,
     "grade_id": "cell-0234315b312d29c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For instance, for two points $\\boldsymbol{x} = \\pmatrix{1 \\\\ 2}$ $\\boldsymbol{y} = \\pmatrix{3 \\\\ 1}$ in $\\mathbf{D}$, we have:\n",
    "$$K(\\boldsymbol{x}, \\boldsymbol{y}) = 1 \\times 3 + 2 \\times 1 = 5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "437234b8294baed41e3451e21d938de1",
     "grade": false,
     "grade_id": "cell-23eb5785fe7568d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Note**: The linear kernel is not really the most useful in practice; \"linear algorithms\" won't work better on \"nonlinear problems\" using it. But it's a useful kernel to check some theoretical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b5d9d7b59d56d581955633da5544312",
     "grade": false,
     "grade_id": "cell-5e247e3a2d2f248c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### The polynomial kernel\n",
    "\n",
    "The *inhomogeneous polynomial kernel* of degree $q$ is defined as: $K_q(\\boldsymbol{x}, \\boldsymbol{y}) = \\left( c + \\boldsymbol{x}^{\\top} \\boldsymbol{y} \\right)^q$, <br>\n",
    "with $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathcal{X}$ and some constant $c \\geq 0$.\n",
    "$K$ is an *homogeneous polynomial kernel* when $c=0$.\n",
    "\n",
    "The feature mapping $\\phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^\\hat{d}$ associated to the polynomial kernel is:\n",
    "$$\n",
    "\\phi(\\boldsymbol{x}) = \\pmatrix{\\cdots & \\sqrt{\\pmatrix{q \\\\ \\boldsymbol{n}} c^{n_0}} \\prod_{k=1}^{d} x_k^{n_k} & \\cdots} \n",
    "$$\n",
    "where the variable $\\boldsymbol{n} = (n_0, \\dots, n_d)$ is the vector of non-negative integers such that $\\sum_{i=0}^d n_i = q$,\n",
    "and where $\\pmatrix{q \\\\ \\boldsymbol{n}}$ denotes the multinomial coefficient:\n",
    "\n",
    "$$\n",
    "\\pmatrix{q \\\\ \\boldsymbol{n}} = \\pmatrix{q \\\\ n_0, n_1, \\dots, n_d} = \\frac{q!}{n_0! n_1! \\dots n_d!}\n",
    "$$\n",
    "\n",
    "The dimensionality of the feature space is $\\hat{d} = \\pmatrix{d + q \\\\ q}$.\n",
    "\n",
    "For instance, for two points $\\boldsymbol{x} = \\pmatrix{1 \\\\ 2}$, $\\boldsymbol{y} = \\pmatrix{3 \\\\ 1}$ in $\\mathbf{D}$ and for the *homogeneous quadratic kernel* ($q=2$ and $c=0$) we have:\n",
    "$$K_2(\\boldsymbol{x}, \\boldsymbol{y}) = (1 \\times 3 + 2 \\times 1)^2 = 25$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f92ab35492114f7a1304d052190f53b3",
     "grade": false,
     "grade_id": "cell-1f6b8670784a6003",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### The Gaussian kernel\n",
    "\n",
    "The Gaussian kernel is defined as:\n",
    "$$\n",
    "K(\\boldsymbol{x}, \\boldsymbol{y}) = \\exp\\left( \\frac{- ||\\boldsymbol{x} - \\boldsymbol{y}||^2}{2 \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "where $||\\boldsymbol{x} - \\boldsymbol{y}||_2^2$ is the Euclidean distance between $\\boldsymbol{x}$ and $\\boldsymbol{y}$\n",
    "and where $\\sigma > 0$ is a given \"spread\", or \"bandwidth\" parameter that plays the same role as the standard deviation in a normal density function.\n",
    "\n",
    "Note that $K(\\boldsymbol{x}, \\boldsymbol{x}) = 1$ and that the kernel value is inversely related to the distance between the two points $\\boldsymbol{x}$ and $\\boldsymbol{y}$.\n",
    "\n",
    "The feature mapping $\\phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{\\infty}$ associated to the Gaussian kernel **has infinite dimensions**, thus we cannot explicitly transform $\\boldsymbol{x}$ into $\\phi(\\boldsymbol{x})$ (see [What about inifinite dimensions?](https://medium.com/analytics-vidhya/the-kernel-trick-and-infinite-dimensions-7ecd91cee6ef)).\n",
    "However computing the Gaussian kernel $K(\\boldsymbol{x}, \\boldsymbol{y})$ is straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfa3bb5973947cbe287bbcba1830cdb4",
     "grade": false,
     "grade_id": "cell-161d0e81b0bd7d43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For instance, for two points $\\boldsymbol{x} = \\pmatrix{1 \\\\ 2}$, $\\boldsymbol{y} = \\pmatrix{3 \\\\ 1}$ in $\\mathbf{D}$ and the *Gaussian kernel* (with $\\sigma=1$) we have\n",
    "$K(\\boldsymbol{x}, \\boldsymbol{y}) \\approx 0.082$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "988dceb4c3c70219c53e96f1d290c5b3",
     "grade": false,
     "grade_id": "cell-58c3482fae111507",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "Implement in Python the following kernels:\n",
    "- Linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df87d18712aa3f69a6b23c019cee64ce",
     "grade": false,
     "grade_id": "cell-937a8eb9746467ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Remark**: We use classes to implement kernels as [*functors*](https://en.wikipedia.org/wiki/Function_object#In_Python) (parametric functions i.e. classes with a defined `__call__` method), see [`lab_session_06`](https://adimajo.github.io/CSE204-2021/lab_session_06/lab_session_06.html) and [`lab_session_07`](https://adimajo.github.io/CSE204-2021/lab_session_07/lab_session_07.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2b3acbbd1a12c99af55020dd192f38f",
     "grade": false,
     "grade_id": "cell-ced13eb7e4e65f6a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LinearKernel:\n",
    "    \"\"\"\n",
    "    The linear kernel is defined as:\n",
    "    \n",
    "    ..math::\n",
    "        K(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x}^{\\top} \\mathbf{y}\n",
    "\n",
    "    The feature mapping associated to this kernel is the identity mapping :math:`\\phi(\\mathbf{x}) \\mapsto \\mathbf{x}`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x1 : numpy.ndarray\n",
    "        a point of the input space (1D numpy array)\n",
    "    x2 : numpy.ndarray\n",
    "        a point of the input space (1D numpy array)\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    k : float\n",
    "        the similarity K(x1, x2)\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        __str__ is called whenever you call str(object_of_that_class)\n",
    "        \"\"\"\n",
    "        return \"Linear\"\n",
    "\n",
    "    def __call__(self, x1: np.ndarray, x2: np.ndarray):\n",
    "        \"\"\"\n",
    "        Compute the Gaussian Kernel\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x1 : numpy.ndarray\n",
    "            a point of the input space (1D numpy array)\n",
    "        x2 : numpy.ndarray\n",
    "            a point of the input space (1D numpy array)\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        k : float\n",
    "            the similarity K(x1, x2)\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "728a8db14295d24a0ba016d1edbb65a9",
     "grade": true,
     "grade_id": "cell-ad73c97c0025d192",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f6c672fd3736f53a1715bc92bb34ae9",
     "grade": false,
     "grade_id": "cell-3bdb087cccafe418",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- Polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c1dfc81979fb804ce4cda66c25e84fd",
     "grade": false,
     "grade_id": "cell-1e4f7ea0013a6577",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class PolynomialKernel:\n",
    "    \"\"\"\n",
    "    The *inhomogeneous polynomial kernel* of degree $q$ is defined as:\n",
    "\n",
    "    ..math::\n",
    "        K_q(\\mathbf{x}, \\mathbf{y}) = \\left( c + \\mathbf{x}^{\\top} \\mathbf{y} \\right)^q\n",
    "\n",
    "    with :math:`\\mathbf{x}, \\mathbf{y} \\in \\mathcal{X}` and some constant :math:`c \\geq 0`.\n",
    "    :math:`K` is an *homogeneous polynomial kernel* when :math:`c=0`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    q : float\n",
    "        the degree of the polynomial kernel\n",
    "    c : float\n",
    "        the polynomial constant\n",
    "    \"\"\"\n",
    "    def __init__(self, q: int = 2, c: int = 0):\n",
    "        \"\"\"\n",
    "        Initialize the inhomogeneous polynomial kernel with values for q (degree) and c (constant) - see above\n",
    "        \"\"\"\n",
    "        self.q = q\n",
    "        self.c = c\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Polynomial\"\n",
    "\n",
    "    def __call__(self, x1: np.ndarray, x2: np.ndarray):\n",
    "        \"\"\"\n",
    "        Compute the polynomial Kernel\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x1 : numpy.ndarray\n",
    "            a point of the input space (1D numpy array)\n",
    "        x2 : numpy.ndarray\n",
    "            a point of the input space (1D numpy array)\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        k : float\n",
    "            the similarity K(x1, x2)\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4071a8e705c6165a4c80376b7965200",
     "grade": true,
     "grade_id": "cell-3af32af7abd682c7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83779d210ea6b86ae3531f5f282a9d12",
     "grade": false,
     "grade_id": "cell-a5cdc3537e5b60ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- Gaussian kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30341d0aa4e518fb5dc06e85586ee807",
     "grade": false,
     "grade_id": "cell-53541c72a7fa8cde",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GaussianKernel:\n",
    "    \"\"\"\n",
    "    The Gaussian kernel is defined as:\n",
    "\n",
    "    ..math::\n",
    "        K(\\boldsymbol{x}, \\boldsymbol{y}) = \\exp \\left( \\frac{- ||\\boldsymbol{x} - \\boldsymbol{y}||_2^2}{2 \\sigma^2} \\right)\n",
    "\n",
    "    where :math:`||\\boldsymbol{x} - \\boldsymbol{y}||_2^2` is the Euclidean distance between :math:`x_1` and :math:`x_2`,\n",
    "    and where :math:`\\sigma > 0` is a given \"spread\" parameter that plays the same role as the standard deviation in a normal density function.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    sigma : float\n",
    "        spread or bandwidth parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma: float = 1.0):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Gaussian\"\n",
    "\n",
    "    def __call__(self, x1: np.ndarray, x2: np.ndarray):\n",
    "        \"\"\"Compute the Gaussian Kernel\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x1 : ndarray\n",
    "            a point of the input space (1D numpy array)\n",
    "        x2 : ndarray\n",
    "            a point of the input space (1D numpy array)\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        k : float\n",
    "            the similarity K(x1, x2)\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d74d5373a2cde2541d19aa2954bd6b4",
     "grade": true,
     "grade_id": "cell-e55144cac553b428",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07a45eefa3b6dfa8c1c3f94764dab581",
     "grade": true,
     "grade_id": "cell-e561c3bccc8872b8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x1 = np.array([1, 2])\n",
    "x2 = np.array([3, 1])\n",
    "\n",
    "kernel = LinearKernel()\n",
    "print(kernel(x1, x2))\n",
    "assert kernel(x1, x2) == 5\n",
    "\n",
    "kernel = PolynomialKernel(q=2, c=0)\n",
    "print(kernel(x1, x2))\n",
    "assert kernel(x1, x2) == 25\n",
    "\n",
    "kernel = GaussianKernel(sigma=1)\n",
    "print(kernel(x1, x2))\n",
    "assert round(kernel(x1, x2), 3) == 0.082"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3849f5f3233ff2013afbaf279deedc52",
     "grade": false,
     "grade_id": "cell-111edd11830d63ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "Implement in Python the following function to make a kernel matrix from a given kernel function $K$ and a given dataset $\\mathbf{D}$.\n",
    "\n",
    "**Reminder**: in Python, a function is an object that can be passed as an argument to a function like any variable (cf. the implementation of `linear_kernel_matrix`, `polynomial_kernel_matrix` and `gaussian_kernel_matrix` hereafter).\n",
    "\n",
    "*Hints*: you can create an empty square matrix with `np.zeros`, `np.ones`, `np.full`... You can do a double for-loop to populate it.\n",
    "\n",
    "**Bonus**: explore [this webpage](https://numpy.org/doc/stable/reference/generated/numpy.ufunc.outer.html) to vectorize your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec12dae40108337a614119c38160f39a",
     "grade": false,
     "grade_id": "cell-90a532176f8f4ed0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def make_kernel_matrix(data: np.ndarray, kernel_function) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Make a Kernel matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : ndarray\n",
    "        the dataset (2D numpy array) with examples in rows and dimensions in columns\n",
    "    kernel_function : function\n",
    "        the kernel function used to make the kernel matrix\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    K : ndarray\n",
    "        the n x n kernel matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b816930d7643d93777e019546216dc38",
     "grade": false,
     "grade_id": "cell-9332626b5afc1923",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The following functions are provided here for convenience with default parameters\n",
    "\n",
    "def linear_kernel_matrix(data):\n",
    "    return make_kernel_matrix(data, LinearKernel())\n",
    "\n",
    "def polynomial_kernel_matrix(data):\n",
    "    return make_kernel_matrix(data, PolynomialKernel())\n",
    "\n",
    "def gaussian_kernel_matrix(data):\n",
    "    return make_kernel_matrix(data, GaussianKernel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "032cde6afecb71a5f46ac4ca66d69e3e",
     "grade": true,
     "grade_id": "cell-bbdda7b27895dd7e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check on a dataset of 3 points in 2 dimensions\n",
    "data = np.array([[1., 2.],\n",
    "                 [3., 1.],\n",
    "                 [2., 3.]])\n",
    "\n",
    "print(linear_kernel_matrix(data), \"\\n\")\n",
    "print(polynomial_kernel_matrix(data), \"\\n\")\n",
    "print(gaussian_kernel_matrix(data), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80554f2284954fd187bd8bbf34cc2611",
     "grade": true,
     "grade_id": "cell-4dce1ea126175609",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c867bc94f3710b410a606c2aec1d0a73",
     "grade": false,
     "grade_id": "cell-24dcb3e8a934f510",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "### Exercise 3: Basic kernel operations in feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0025cfa7c18944e758d7f15188e42790",
     "grade": false,
     "grade_id": "cell-9faf7c5779b5c361",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this exercise, we will look at some of the basic operations that can be performed in the feature space solely via kernels (i.e. without computing $\\phi(\\boldsymbol{x})$).\n",
    "These are the standard building blocks that are used to *kernelize* algorithms like k-Means, PCA, ...\n",
    "\n",
    "We will see that we can get the norm of a point in the feature space, the distance between two points, the mean of the training dataset and the variance respectively. We will proceed by centering, and optionnally normalize points in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60a64cb8b038080aace2e5d506afb229",
     "grade": false,
     "grade_id": "cell-5fe939585b6961a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Norm of a point\n",
    "\n",
    "**Question 1:**\n",
    "Show that we can compute the norm of a point $\\phi(\\boldsymbol{x})$ in the feature space as follows:\n",
    "$$\n",
    "\\left\\lVert \\phi(\\boldsymbol{x}) \\right\\rVert = \\sqrt{K(\\boldsymbol{x}, \\boldsymbol{x})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e853beddaf93e7b82f7ca3119e20e0b",
     "grade": true,
     "grade_id": "cell-8b80565232397e08",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17c9634f8216308886fb2537f665d920",
     "grade": false,
     "grade_id": "cell-0473a0128bc8f358",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Distance between points\n",
    "\n",
    "**Question 2:**\n",
    "Show that the distance between two points $\\phi(\\boldsymbol{x}_i)$ and $\\phi(\\boldsymbol{x}_j)$ in the feature space can be computed as:\n",
    "$$\n",
    "\\left\\lVert \\phi(\\boldsymbol{x}_i) - \\phi(\\boldsymbol{x}_j) \\right\\rVert = \\sqrt{K(\\boldsymbol{x}_i, \\boldsymbol{x}_i) + K(\\boldsymbol{x}_j, \\boldsymbol{x}_j) - 2 K(\\boldsymbol{x}_i, \\boldsymbol{x}_j)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "410e4aaa652fdcce3c97121e0bbfde03",
     "grade": true,
     "grade_id": "cell-d6622b29ac6a7149",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e5ffe5c8bd8a6175faaeb17778b28e9",
     "grade": false,
     "grade_id": "cell-e29a0a34662291a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Mean in feature space\n",
    "\n",
    "**Question 3:**\n",
    "Show that the squared norm of the mean vector $\\boldsymbol{\\mu}_{\\phi}$ consisting of the mean of each coordinate in the feature space for all training points $\\boldsymbol{x}_i$ for $1 \\leq i \\leq n$ can be computed as:\n",
    "$$\n",
    "\\left\\lVert \\boldsymbol{\\mu}_{\\phi} \\right\\rVert^2\n",
    "= \\boldsymbol{\\mu}_{\\phi}^{\\top} \\boldsymbol{\\mu}_{\\phi}\n",
    "= \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n K(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\n",
    "$$\n",
    "\n",
    "In other words, show that the squared norm of the mean in feature space is simply the average of the values in the kernel matrix $\\mathbf{K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c66e2dd51c4a4b883ddf9a5b8c867325",
     "grade": true,
     "grade_id": "cell-1945f19850a643e2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e129a251b70e92921e8c14c614c8296f",
     "grade": false,
     "grade_id": "cell-da0f9f87a2058f47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Total variance in feature space\n",
    "\n",
    "**Question 4:**\n",
    "Show that the total variance $\\sigma_{\\phi}^2$ of the training points $\\boldsymbol{x}_i$ in the feature space can be computed as:\n",
    "$$\n",
    "\\sigma_{\\phi}^2\n",
    "= \\frac{1}{n} \\sum_{i=1}^n \\left\\lVert \\phi(\\boldsymbol{x}_i) - \\boldsymbol{\\mu}_{\\phi} \\right\\rVert^2\n",
    "= \\frac{1}{n} \\sum_{i=1}^n K(\\boldsymbol{x}_i, \\boldsymbol{x}_i) - \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n K(\\boldsymbol{x}_i, \\boldsymbol{x}_j).\n",
    "$$\n",
    "\n",
    "In other words, show that the total variance in feature space is simply the difference between the average of the diagonal entries and the average of the entire kernel matrix $\\mathbf{K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "948346ce5db41a6fb974c20ff55e595e",
     "grade": true,
     "grade_id": "cell-8b07748e178cae45",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "80cbdcdaeeffbb91bc9585b96fd89e58",
     "grade": false,
     "grade_id": "cell-7a558772b1a289fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Centering in feature space\n",
    "\n",
    "We can center each point in the feature space by subtracting the mean from it: $\\tilde{\\phi}(\\boldsymbol{x}_i) = \\phi(\\boldsymbol{x}_i) - \\boldsymbol{\\mu}_{\\phi}$.\n",
    "\n",
    "Remember that we don't have an explicit representation of $\\phi(\\mathbf{x}_i)$ or $\\boldsymbol{\\mu}_{\\phi}$\n",
    "but we can still compute the *centered kernel matrix* $\\tilde{\\mathbf{K}}$, that is the kernel matrix over centered points: $\\tilde{\\mathbf{K}} = \\left\\{ \\tilde{K}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\right\\}^n_{i, j=1}$\n",
    "where each element corresponds to the kernel between centered points $\\tilde{K}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\tilde{\\phi}(\\boldsymbol{x}_i)^{\\top} \\tilde{\\phi}(\\boldsymbol{x}_j)$.\n",
    "\n",
    "**Question 5:**\n",
    "Show that:\n",
    "$$\n",
    "\\tilde{K}(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\n",
    "= K(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\n",
    "- \\frac{1}{n} \\sum_{k=1}^n K(\\boldsymbol{x}_i, \\boldsymbol{x}_k)\n",
    "- \\frac{1}{n} \\sum_{k=1}^n K(\\boldsymbol{x}_j, \\boldsymbol{x}_k)\n",
    "+ \\frac{1}{n^2} \\sum_{a=1}^n \\sum_{b=1}^n K(\\boldsymbol{x}_a, \\boldsymbol{x}_b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76b190eaa65755d774a557809bc0a054",
     "grade": true,
     "grade_id": "cell-7b3944690a85ab76",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5acc46dff8cb794da3e1b4cd24f40b33",
     "grade": false,
     "grade_id": "cell-d870345231ab1ab4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Note**: this can be rewritten in matrix form as:\n",
    "\\begin{align}\n",
    "\\tilde{\\mathbf{K}}\n",
    "& =\n",
    "  \\mathbf{K}\n",
    "- \\frac{1}{n} ~ \\mathbf{1}_{n \\times n} ~ \\mathbf{K}\n",
    "- \\frac{1}{n} ~ \\mathbf{K} ~ \\mathbf{1}_{n \\times n}\n",
    "+ \\frac{1}{n^2} ~ \\mathbf{1}_{n \\times n} ~ \\mathbf{K} ~ \\mathbf{1}_{n \\times n} \\\\\n",
    "& = \\left( \\mathbf{I} - \\frac{1}{n} \\mathbf{1}_{n \\times n} \\right) \\mathbf{K} \\left( \\mathbf{I} - \\frac{1}{n} \\mathbf{1}_{n \\times n} \\right)\n",
    "\\end{align}\n",
    "where $\\mathbf{I}$ is the $n \\times n$ identity matrix\n",
    "and $\\mathbf{1}_{n \\times n}$ is the $n \\times n$ matrix with all elements equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6832c2725d7b83ac7106cdc938393cce",
     "grade": false,
     "grade_id": "cell-4e2b1a9801656c6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Normalizing in feature space\n",
    "\n",
    "One way to normalize points in a feature space is to ensure that they have a unit length by replacing $\\phi(\\boldsymbol{x}_i)$ with the corresponding unit vector $\\phi_n(\\boldsymbol{x}_i) = \\frac{\\phi(\\boldsymbol{x}_i)}{\\lVert \\phi(\\boldsymbol{x}_i) \\rVert}$.\n",
    "\n",
    "The dot product in a normalized feature space corresponds to the cosine of the angle between the two mapped points because\n",
    "$$\n",
    "\\phi_n(\\boldsymbol{x}_i)^{\\top} \\phi_n(\\boldsymbol{x}_j)\n",
    "= \\frac{\\phi(\\boldsymbol{x}_i)^{\\top} \\phi(\\boldsymbol{x}_j)}{\\lVert \\phi(\\boldsymbol{x}_i) \\rVert \\cdot \\lVert \\phi(\\boldsymbol{x}_j) \\rVert}\n",
    "= \\cos(\\theta)\n",
    "$$\n",
    "\n",
    "If the mapped points are both centered and normalized, then a dot product corresponds to the correlation between the two points in the feature space.\n",
    "\n",
    "The normalized kernel matrix $\\mathbf{K}_n$ can be computed as:\n",
    "$$\n",
    "\\mathbf{K}_n( \\boldsymbol{x}_i, \\boldsymbol{x}_j )\n",
    "= \\frac{\\phi(\\boldsymbol{x}_i)^{\\top} \\phi(\\boldsymbol{x}_j)}{\\lVert \\phi(\\boldsymbol{x}_i) \\rVert \\cdot \\lVert \\phi(\\boldsymbol{x}_j) \\rVert}\n",
    "= \\frac{K(\\boldsymbol{x}_i, \\boldsymbol{x}_j)}{\\sqrt{K(\\boldsymbol{x}_i, \\boldsymbol{x}_i) \\cdot K(\\boldsymbol{x}_j, \\boldsymbol{x}_j)}}\n",
    "$$\n",
    "\n",
    "$\\mathbf{K}_n$ has diagonal elements equal to 1.\n",
    "\n",
    "**Note**: this can be rewritten as:\n",
    "$$\n",
    "\\mathbf{K}_n = \\mathbf{W}^{-1/2} \\cdot \\mathbf{K} \\cdot \\mathbf{W}^{-1/2}\n",
    "$$\n",
    "\n",
    "with $\\mathbf{W}^{-1/2}$ the diagonal matrix defined as\n",
    "$$\n",
    "\\large\n",
    "\\mathbf{W}^{-1/2} =\n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{\\sqrt{K(\\boldsymbol{x}_1, \\boldsymbol{x}_1)}} & 0                                              & \\cdots & 0\\\\\n",
    "0                                              & \\frac{1}{\\sqrt{K(\\boldsymbol{x}_2, \\boldsymbol{x}_2)}} & \\cdots & 0\\\\\n",
    "\\vdots                                         & \\vdots                                         & \\ddots & \\vdots\\\\\n",
    "0                                              & 0                                              & \\cdots & \\frac{1}{\\sqrt{K(\\boldsymbol{x}_n, \\boldsymbol{x}_n)}}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5f601dc7028691e85e17dde7f190732",
     "grade": false,
     "grade_id": "cell-1418753554bf4607",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2: Clustering with the kernel k-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efe7c6fd1952efcae0efb57a6c305c10",
     "grade": false,
     "grade_id": "cell-ff287ff99f22cb56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1778c839b6b7f5ebe725813d4b96b7f9",
     "grade": false,
     "grade_id": "cell-55476c354e4e6737",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have studied the k-means algorithm for clustering tasks [last week](https://adimajo.github.io/CSE204-2021/lab_session_13/lab_session_13.html).\n",
    "As this algorithm can easily be kernelized, we will use it to make a first implementation of a kernel method: Kernel k-Means.\n",
    "\n",
    "In k-Means, the separating boundary between clusters is linear.\n",
    "As we saw in the previous part, the most obvious advantage of applying the *kernel trick* to k-Means is that it allows us to extract nonlinear boundaries between clusters.\n",
    "In other words, Kernel k-Means can detect nonconvex clusters.\n",
    "\n",
    "You will see that this *Kernel k-Means* can be seen as an alternative to *Spectral Clustering*, the other clustering algorithm [seen last week](https://adimajo.github.io/CSE204-2021/lab_session_13/lab_session_13.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08fc8a8266db4bd0080084a1dcf0f0f8",
     "grade": false,
     "grade_id": "cell-9d5f1834a85d5b61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Once again, the main idea here is to conceptually map data points $\\boldsymbol{x}$ in the input space to a point $\\phi(\\boldsymbol{x})$ in some high-dimensional feature space $\\mathcal{H}$ via an appropriate nonlinear mapping $\\phi$.\n",
    "The kernel trick allows us to carry out the clustering in feature space without explicitly making computations in this feature space.\n",
    "All computation will be done in the input space $\\mathcal{X}$, using a kernel function $K(\\boldsymbol{x}_i, \\mathbf{x}_j)$ which will replace every dot (or inner) product $\\phi(\\boldsymbol{x}_i)^{\\top} \\phi(\\boldsymbol{x}_j)$.\n",
    "This means that we will have to rewrite the k-means algorithm such that the mapping function $\\phi$ only appears in such dot products\n",
    "$\\phi(\\boldsymbol{x}_i)^{\\top} \\phi(\\boldsymbol{x}_j)$ (which in the end will be replaced by $K(\\boldsymbol{x}_i, \\boldsymbol{x}_j)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1ac3fbea66e180669e39747cc82ceb3",
     "grade": false,
     "grade_id": "cell-a76106fbdd44ddea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "187f1fabc74a08a1034461d825a2191b",
     "grade": false,
     "grade_id": "cell-480f5ed717ac7dc6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We (still) denote by $\\boldsymbol{x} \\in \\mathbb{R}^d$ the point in the input space and\n",
    "$\\phi(\\boldsymbol{x})$ its corresponding image in the feature space.\n",
    "$\\mathcal{D} = (\\boldsymbol{x}_i)_1^n \\subset \\mathbb{R}^d$ is our dataset containing $n$ points.\n",
    "We want to define $k$ clusters $\\boldsymbol{\\mathcal{C}} = \\{\\boldsymbol{C}_1, \\dots, \\boldsymbol{C}_k\\}$, each one defining the cluster mean / centroid $\\{\\boldsymbol{\\mu}^{\\phi}_1, \\dots, \\boldsymbol{\\mu}^{\\phi}_k\\}$ in the feature space:\n",
    "$$\n",
    "\\boldsymbol{\\mu}^{\\phi}_i = \\frac{1}{n_i} \\sum_{\\boldsymbol{x}_j \\in C_i} \\phi(\\boldsymbol{x}_j)\n",
    "$$\n",
    "where $\\boldsymbol{\\mu}^{\\phi}_i$ is the mean of the $i$th cluster $\\boldsymbol{C}_i$ in feature space,\n",
    "with $n_i = |\\boldsymbol{C}_i|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59adb08fdd8ff1e931e6b1028c96690e",
     "grade": false,
     "grade_id": "cell-368983432ffdc3d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 1: express the k-means *sum of squared errors* (SSE) objective function in terms of the kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4208a7ea26207f76166b4d5342ba04c2",
     "grade": false,
     "grade_id": "cell-573e008dbe3dbdb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The kernel k-means sum of squared errors (SSE) objective can be written as\n",
    "$$\n",
    "SSE(\\boldsymbol{\\mathcal{C}}) = \\sum_{i=1}^k\\sum_{\\boldsymbol{x}_j \\in \\boldsymbol{C}_i} \\left\\lVert \\phi(\\boldsymbol{x}_j) - \\boldsymbol{\\mu}^{\\phi}_i \\right\\rVert^2\n",
    "$$\n",
    "\n",
    "**Question 1**: show that $SSE(\\boldsymbol{\\mathcal{C}})$ can be rewritten as:\n",
    "$$\n",
    "SSE(\\boldsymbol{\\mathcal{C}}) =\n",
    "\\left( \\sum_{i=1}^k\\sum_{\\boldsymbol{x}_j \\in \\boldsymbol{C}_i} \\phi(\\boldsymbol{x}_j)^{\\top} \\phi(\\boldsymbol{x}_j) \\right)\n",
    "- \\left( \\sum_{i=1}^k n_i \\left\\lVert \\boldsymbol{\\mu}^{\\phi}_i \\right\\rVert^2 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97a3ded1ae826f9d7dbeed2f4fce6e8d",
     "grade": true,
     "grade_id": "cell-31b06c5e792e3dc5",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "897a55f0413ff429517758d2b65be0c8",
     "grade": false,
     "grade_id": "cell-84ce232f8213b893",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2**: show then - using kernel operations seen in Part 1 - that $SSE(\\mathcal{C})$ can be rewritten as:\n",
    "$$\n",
    "SSE(\\boldsymbol{\\mathcal{C}}) =\n",
    "\\sum_{j=1}^n K(\\boldsymbol{x}_j, \\boldsymbol{x}_j)\n",
    "- \\sum_{i=1}^k \\frac{1}{n_i}\n",
    "  \\sum_{\\boldsymbol{x}_a \\in \\boldsymbol{C}_i}\n",
    "  \\sum_{\\boldsymbol{x}_b \\in \\boldsymbol{C}_i} K(\\boldsymbol{x}_a, \\boldsymbol{x}_b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf349ef0b4fb355a44e5bbe1c163aad5",
     "grade": true,
     "grade_id": "cell-b2c0871dc6b8024a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ecb9da498430691519f1f2d576048d44",
     "grade": false,
     "grade_id": "cell-883da224eba09df5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As for k-Means, we assign in Kernel k-Means each point to the closest mean / centroid in feature space, resulting in a new clustering, which in turn can be used to obtain new estimates for the cluster means.\n",
    "The main difficulty is that we cannot explicitly compute the mean of clusters in feature space.\n",
    "Fortunately, we will see that knowing explicitly the mean of clusters is not required for the clustering task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e15b37522d3cdac00627ba20a14d1c9",
     "grade": false,
     "grade_id": "cell-987a0712a19d9bb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 2: express the distance of a point $\\phi(\\boldsymbol{x}_j)$ to the mean $\\boldsymbol{\\mu}^{\\phi}_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "553df555c9eb4c46c2df12e539552ace",
     "grade": false,
     "grade_id": "cell-862c7b59e4b363f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 3**: show that\n",
    "$$\n",
    "\\left\\lVert \\phi(\\boldsymbol{x}_j) - \\boldsymbol{\\mu}^{\\phi}_i \\right\\rVert^2\n",
    "=\n",
    "K(\\boldsymbol{x}_j, \\boldsymbol{x}_j)\n",
    "- \\frac{2}{n_i} \\sum_{\\boldsymbol{x}_a \\in \\boldsymbol{C}_i} K(\\boldsymbol{x}_a, \\boldsymbol{x}_j)\n",
    "+ \\frac{1}{n^2_i} \\sum_{\\boldsymbol{x}_a \\in \\boldsymbol{C}_i}\\sum_{\\mathbf{x}_b \\in \\boldsymbol{C}_i} K(\\boldsymbol{x}_a, \\boldsymbol{x}_b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfc1918294a58b23f3e6c6bcebdc2920",
     "grade": true,
     "grade_id": "cell-5bf40b52dea326f6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6425f9983cdf8bc485842e471db8784",
     "grade": false,
     "grade_id": "cell-c7eb9cf2961826f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Thanks to this equation, the distance of a point to a cluster mean in feature space can be computed using only kernel operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9fae203beb0d959f5b1ba83ca0dc5dea",
     "grade": false,
     "grade_id": "cell-0c7ed27ad38c412d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 3: define the cluster assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac52b62687aa04e4548d2e3cbcfac9d4",
     "grade": false,
     "grade_id": "cell-9329970aed2d65b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 4**: show that the cluster assignment used at each iteration of the k-means algorithm can be written as follows\n",
    "$$\n",
    "\\boldsymbol{C}^*(\\boldsymbol{x}_j) =\n",
    "\\arg\\!\\min_i \\left\\{\n",
    "\\frac{1}{n^2_i} \\sum_{\\boldsymbol{x}_a \\in \\boldsymbol{C}_i}\\sum_{\\boldsymbol{x}_b \\in \\boldsymbol{C}_i} K(\\boldsymbol{x}_a, \\boldsymbol{x}_b)\n",
    "- \\frac{2}{n_i} \\sum_{\\boldsymbol{x}_a \\in \\boldsymbol{C}_i} K(\\boldsymbol{x}_a, \\boldsymbol{x}_j)\n",
    "\\right\\}.\n",
    "$$\n",
    "\n",
    "*Hint:* recall that, in the input space, the cluster assignment is\n",
    "$$\\boldsymbol{C}^*(\\boldsymbol{x}_j) =\n",
    "\\arg\\!\\min_i \\left\\{\n",
    "\\left\\lVert \\boldsymbol{x}_j - \\boldsymbol{\\mu}_i \\right\\rVert^2\n",
    "\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6db35812ac76ddc41a62f723e641ab6",
     "grade": true,
     "grade_id": "cell-e7a088427fd04067",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d0042b961c389e4853c5ca3859b331e",
     "grade": false,
     "grade_id": "cell-b15edf4bd4745f05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that the first term in the argmin is the average pairwise kernel value for cluster $\\boldsymbol{C}_i$ and it is independent of the point $\\boldsymbol{x}_j$.\n",
    "It is the squared norm of the cluster mean in feature space.\n",
    "\n",
    "The second term is twice the average kernel value for points in $\\boldsymbol{C}_i$ with respect to $\\boldsymbol{x}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a1e054050ea4eaf01d8d3903d736818",
     "grade": false,
     "grade_id": "cell-3c5e5e06a5d37e2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 4: implement Kernel K-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9fd504ae386a04506b2594e99a24b35",
     "grade": false,
     "grade_id": "cell-adb81b14ea74fdd1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 5**: implement in Python Kernel K-means defined below.\n",
    "\n",
    "___\n",
    "### Algorithm 1\n",
    "\n",
    "Kernel K-means$(\\mathbf{K}, k, \\epsilon)$\n",
    "\n",
    "$t \\leftarrow 0$ <br>\n",
    "$\\boldsymbol{\\mathcal{C}} \\leftarrow \\{\\boldsymbol{C}_1, \\dots, \\boldsymbol{C}_k\\} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad$ # Randomly partition points into $k$ clusters <br>\n",
    "\n",
    "**REPEAT**\n",
    "\n",
    "$\\quad t \\leftarrow t+1$ <br>\n",
    "\n",
    "$\\quad$**FOREACH** $\\boldsymbol{C}_i \\in \\boldsymbol{\\mathcal{C}}^{t-1}$ <br>\n",
    "$\\quad\\quad$$\\text{sqnorm}_i \\leftarrow \\frac{1}{n^2_i} \\sum_{\\boldsymbol{x}_a \\in \\boldsymbol{C}_i}\\sum_{\\boldsymbol{x}_b \\in \\boldsymbol{C}_i} K(\\boldsymbol{x}_a, \\boldsymbol{x}_b) \\quad\\quad\\quad$ # Compute the squared norm of cluster means <br>\n",
    "\n",
    "$\\quad$**FOREACH** $\\boldsymbol{x}_i \\in \\mathcal{D}$ <br>\n",
    "$\\quad\\quad$**FOREACH** $\\boldsymbol{C}_i \\in \\boldsymbol{\\mathcal{C}}^{t-1}$ <br>\n",
    "$\\quad\\quad\\quad$$\\text{avg}_{ji} \\leftarrow \\frac{1}{n_i} \\sum_{\\boldsymbol{x}_a \\in \\boldsymbol{C}_i} K(\\boldsymbol{x}_a, \\boldsymbol{x}_j) \\quad\\quad\\quad\\quad\\quad\\quad$ # Average kernel value for $\\boldsymbol{x}_j$ and $\\boldsymbol{C}_i$ <br>\n",
    "\n",
    "$\\quad$# Find the closest cluster for each point <br>\n",
    "$\\quad$**FOREACH** $\\boldsymbol{x}_i \\in \\mathcal{D}$ <br>\n",
    "$\\quad\\quad$**FOREACH** $\\boldsymbol{C}_i \\in \\mathcal{C}^{t-1}$ <br>\n",
    "$\\quad\\quad\\quad$$d(\\boldsymbol{x}_j, \\boldsymbol{C}_i) \\leftarrow \\text{sqnorm}_i - 2 ~ \\text{avg}_{ji}$ <br>\n",
    "$\\quad\\quad$$j^* \\leftarrow \\arg\\!\\min_i \\left\\{ d(\\boldsymbol{x}_j, \\boldsymbol{C}_i) \\right\\}$ <br>\n",
    "$\\quad\\quad$$\\boldsymbol{C}^t_{j^*} \\leftarrow \\boldsymbol{C}^t_{j^*} \\cup \\{ \\boldsymbol{x}_j \\} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad$ # Cluster reassignment <br>\n",
    "\n",
    "$\\quad$$\\boldsymbol{\\mathcal{C}}^t \\leftarrow \\{\\boldsymbol{C}_1^t, \\dots, \\boldsymbol{C}_k^t\\}$ <br>\n",
    "\n",
    "**UNTIL** the fraction of points with new cluster assignments $\\leq \\epsilon$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a56bf562de919f17ab85038f96229a9f",
     "grade": false,
     "grade_id": "cell-2cf538a50e2b5d3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Complete functions `init_points_cluster_index` and `kernel_k_means` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56905e6a8c4300119084d8c8f1e7ac71",
     "grade": false,
     "grade_id": "cell-3bb79d305ce4e0c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def take(n: int, iterable) -> list:\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(itertools.islice(iterable, n))\n",
    "\n",
    "def init_points_cluster_index(n: int, k: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Randomly partition n points into k clusters\n",
    "\n",
    "    :param int n: number of rows / samples of the resulting array\n",
    "    :param int k: number of clusters\n",
    "    :returns: 1D array of length n with random values 0 ... k-1 representing initial cluster values\n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a435e1bca9efde4c4c7beaae8e37237",
     "grade": true,
     "grade_id": "cell-94102d461b0eacce",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test your implementation of init_points_cluster_index\n",
    "assert init_points_cluster_index(3, 2).shape == (3,)\n",
    "assert (0 <= init_points_cluster_index(3, 2)).all()\n",
    "assert (init_points_cluster_index(3, 2) <= 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ea4c7d7b0c570131bbcf666abcb1932",
     "grade": false,
     "grade_id": "cell-95237f8c9fcc3d53",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def kernel_k_means(dataset: np.ndarray, K: np.ndarray, k: int, epsilon: float = 0):\n",
    "    \"\"\"\n",
    "    Kernel K-means\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : numpy.ndarray\n",
    "        the dataset to perform kernel k means on\n",
    "    K : numpy.ndarray\n",
    "        the n-by-n Kernel matrix of inputs\n",
    "    k : int\n",
    "        the number of clusters to find\n",
    "    epsilon : float\n",
    "        the termination criterion\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    points_cluster_index : numpy.ndarray\n",
    "        a 1D vector of labels of length n (e.g. c[i] = C_j means \"x_i is belongs to cluster C_j\")\n",
    "    iterations : int\n",
    "        the number of iterations until convergence\n",
    "    loss : float\n",
    "        the achieved loss\n",
    "    \"\"\"\n",
    "\n",
    "    n = K.shape[0]   # number of  points\n",
    "    \n",
    "    if n < k:\n",
    "        raise Exception(\"There are less points than clusters\")\n",
    "\n",
    "    # Init points cluster\n",
    "    points_cluster_index = init_points_cluster_index(n, k)\n",
    "\n",
    "    finished = False\n",
    "    iterations = 0\n",
    "    while not finished:\n",
    "        if iterations > 100:\n",
    "            break\n",
    "        iterations += 1\n",
    "        previous_points_cluster_index = points_cluster_index\n",
    "\n",
    "        # Compute \"distances\" of points to clusters\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "            # Number of points in cluster i\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            # Kernel matrix of points belonging to cluster i\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            # Compute the square norm of clusters mean\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            # Kernel matrix of points belonging to cluster i in rows and point j in column\n",
    "            # Average kernel values between point j and cluster i\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        # Assign clusters to points\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return points_cluster_index, iterations, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a537a2688ab3f84e4540cb6dcfb0fee",
     "grade": false,
     "grade_id": "cell-410c51ab5d8c4bcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 5: Test the implementation on a set of given datasets\n",
    "\n",
    "We will use the same datasets than last week:\n",
    "- the first dataset consists of 4 gaussian-distributed clusters of points with equal variance;\n",
    "- the second represents two clusters, one stretched vertically, and one horizontally;\n",
    "- finally, the last dataset represents 3 clusters distributed in rings.\n",
    "\n",
    "For convenience, the three datasets are placed in a list called `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "daa076e04120177e4e2dc6d3effdafc4",
     "grade": false,
     "grade_id": "cell-3a20658d8a1c2631",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a data set\n",
    "N = 120\n",
    "\n",
    "data1 = np.random.normal((0,0), (0.5, 0.5) ,size=(N,2))\n",
    "data1 = np.append(data1, np.random.normal((5,0), (0.5, 0.5), size=(N, 2)), axis=0)\n",
    "data1 = np.append(data1, np.random.normal((0,5), (0.5, 0.5), size=(N, 2)), axis=0)\n",
    "data1 = np.append(data1, np.random.normal((5,5), (0.5, 0.5), size=(N, 2)), axis=0)\n",
    "\n",
    "data2 = np.random.normal((2, 5), (0.25, 1), size=(N, 2))\n",
    "data2 = np.append(data2, np.random.normal((5,5), (1, 0.25), size=(N, 2)), axis=0)\n",
    "\n",
    "# radii = np.random.normal(0, 0.5, size=(N, 1))\n",
    "radii = np.random.normal(0, 1.5, size=(N, 1))\n",
    "# radii = np.append(radii, np.random.normal(4, 0.5, size=(2 * N, 1)), axis=0)\n",
    "radii = np.append(radii, np.random.normal(8, 0.5, size=(3 * N, 1)), axis=0)\n",
    "# angles = np.random.uniform(size=(6 * N, 1)) * 2.0 * np.pi\n",
    "angles = np.random.uniform(size=(4 * N, 1)) * 2.0 * np.pi\n",
    "data3 = np.hstack([radii * np.cos(angles), radii * np.sin(angles)])\n",
    "\n",
    "datasets = [data1, data2, data3]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(datasets), figsize=(10, 3))\n",
    "for i, data in enumerate(datasets):\n",
    "    axes[i].scatter(data[:, 0], data[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b74eaefdf2edfae4a3de1ee78d660f9",
     "grade": false,
     "grade_id": "cell-a62b149919aabbb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To test your implementation, run the following code which will plot the 3 datasets, trying different values of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c26b6369469c6be764c29308a3095f2",
     "grade": true,
     "grade_id": "cell-e94aa7b68dd0ef1e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd271057c67e5c6e778143a62049a833",
     "grade": true,
     "grade_id": "cell-a4bb80c820df2c24",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for kernel in [GaussianKernel(sigma=1.5),\n",
    "               PolynomialKernel(q=2., c=0.),\n",
    "               LinearKernel()]:\n",
    "    print(\"kernel:\", str(kernel))\n",
    "    fig, axes = plt.subplots(3, len(datasets), figsize=(20, 20))\n",
    "\n",
    "    for k_index, k in enumerate([2, 3, 4]):\n",
    "        print(\"k:\", k)\n",
    "        for dataset_index, data in enumerate(datasets):\n",
    "            print(\"dataset:\", dataset_index)\n",
    "            kernel_matrix = make_kernel_matrix(data, kernel)\n",
    "            clusters, iterations, loss = kernel_k_means(data, kernel_matrix, k, epsilon=0)\n",
    "            axes[k_index, dataset_index].scatter(data[:,0], data[:,1], c=clusters, cmap='rainbow')\n",
    "            axes[k_index, dataset_index].set_title('{} kernel k={}  iter={}  loss={:0.1f}'.format(str(kernel), k, iterations, loss))\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fbaf9411fd8f4aef543dcea629d835a",
     "grade": false,
     "grade_id": "cell-16c31384dd31c8fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 3: Feature extraction with the Kernel PCA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be2d808779eefa9736d8fd80e52a6b69",
     "grade": false,
     "grade_id": "cell-b2c3876719db7d9c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have studied the Principal Component Analysis (PCA) algorithm for features extraction tasks in [`lab_session_10`](https://adimajo.github.io/CSE204-2021/lab_session_10/lab_session_10.html).\n",
    "PCA can be \"kernelized\" to find nonlinear \"directions\" in the data.\n",
    "\n",
    "Kernel PCA finds the directions of most variance in the feature space instead of the input space.\n",
    "\n",
    "Again, using the *kernel trick*, all operations can be carried out in terms of the kernel function in the input space, without having to transform the data into feature space.\n",
    "\n",
    "We won't detail the kernelization process here but instead we will focus on the implementation and the obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76fd7a916a2eaabf06cfe5a47992aaa2",
     "grade": false,
     "grade_id": "cell-54cb7336994d61f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As a reminder, the PCA algorithm is described in algorithm 2 defined below. Its kernelized version is defined in algorithm 3.\n",
    "\n",
    "___\n",
    "### Algorithm 2\n",
    "\n",
    "PCA$(\\mathbf{D}, r)$\n",
    "$$\n",
    "\\begin{array}{lrcl}\n",
    "{\\tiny 1.} \\quad\\quad & \\boldsymbol{\\mu}    & = & \\frac{1}{n} \\sum_{i=1}^{n} \\boldsymbol{x}_i                                          & \\text{# Compute the mean} \\\\\n",
    "{\\tiny 2.}            & \\mathbf{Z}      & = & \\mathbf{D} - \\mathbf{1}_{n \\times d} \\cdot \\boldsymbol{\\mu}^{\\top}                                & \\text{# Center the dataset } \\mathbf{D} \\text{ (c.f. note below)} \\\\\n",
    "{\\tiny 3.}            & \\mathbf{\\Sigma} & = & \\frac{1}{n} \\left( \\mathbf{Z}^{\\top} \\mathbf{Z} \\right)                          & \\text{# Compute the covariance matrix} \\\\\n",
    "{\\tiny 4.}          &  \\boldsymbol{\\lambda} =  (\\lambda_1, \\lambda_2, \\dots, \\lambda_d)                         & = & \\text{eigenvalues}(\\mathbf{\\Sigma})  & \\text{# Compute eigenvalues} \\\\\n",
    "{\\tiny 5.}            & \\mathbf{U}   = \\pmatrix{ \\boldsymbol{u}_1 & \\boldsymbol{u}_2 & \\dots & \\boldsymbol{u}_d } & = & \\text{eigenvectors}(\\mathbf{\\Sigma}) & \\text{# Compute eigenvectors} \\\\\n",
    "{\\tiny 6.}            & \\mathbf{U}_r & = & \\pmatrix{\\boldsymbol{u}_1 & \\boldsymbol{u}_2 & \\dots & \\boldsymbol{u}_r}                                   & \\text{# Reduced basis} \\\\\n",
    "{\\tiny 7.}            & \\mathbf{A}   & = & \\left\\{\\boldsymbol{a}_i | \\boldsymbol{a}_i = \\mathbf{U}_r^{\\top}\\boldsymbol{x}_i ~ \\text{ for } i = 1, \\dots, n \\right\\} \\quad  & \\text{# Projected data} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0dd1ff93c85ad8406116e0bc8a917251",
     "grade": false,
     "grade_id": "cell-0c5a43bc9ee1c82a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here, $\\mathbf{D}$ is the $n \\times d$ dataset (or \"design\") matrix ($n$ points of $d$ dimensions) and $r$ is the size of the basis we want to compute.\n",
    "\n",
    "$\\mathbf{1}_{n \\times d}$ is the $n \\times d$ matrix filled with 1s ([numpy.ones(shape=(n, d))](https://numpy.org/doc/stable/reference/generated/numpy.ones.html?highlight=ones#numpy.ones) in Python).\n",
    "\n",
    "**Note:** The $\\mathbf{1} \\cdot \\boldsymbol{\\mu}^{\\top}$ matrix can be made with [np.tile(mu, (d, 1))](https://numpy.org/doc/stable/reference/generated/numpy.tile.html?highlight=tiles) in Python (assuming $\\boldsymbol{\\mu}$ is a 1D numpy array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6794719176a38e93e161754962c1ff9f",
     "grade": false,
     "grade_id": "cell-1272236b12f74ce3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 1**: implement in Python the Kernel PCA algorithm defined below.\n",
    "\n",
    "___\n",
    "### Algorithm 3\n",
    "\n",
    "Kernel PCA$(\\mathbf{D}, K, r)$\n",
    "\n",
    "$$\n",
    "\\begin{array}{lrcl}\n",
    "{\\tiny 1.} \\quad\\quad & \\mathbf{K}   & = & \\{ K(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\}_{i,j=1, \\dots, n}                                                              & \\text{# Compute the } n \\times n \\text{ kernel matrix from the dataset } \\mathbf{D} \\\\\n",
    "{\\tiny 2.}            & \\mathbf{K}   & = & (\\mathbf{I} - \\frac{1}{n}\\mathbf{1}_{n \\times n}) ~ \\mathbf{K} ~ (\\mathbf{I} - \\frac{1}{n}\\mathbf{1}_{n \\times n}) & \\text{# Center the kernel matrix (c.f. Part 1)} \\\\\n",
    "{\\tiny 3.}            & (\\eta_1, \\eta_2, \\dots, \\eta_d)                   & = & \\text{eigenvalues}(\\mathbf{K})                                                & \\text{# Compute eigenvalues} \\\\\n",
    "{\\tiny 4.}            & \\pmatrix{ \\boldsymbol{c}_1 & \\boldsymbol{c}_2 & \\dots & \\boldsymbol{c}_n } & = & \\text{eigenvectors}(\\mathbf{K})                                               & \\text{# Compute eigenvectors} \\\\\n",
    "{\\tiny 5.}            & \\mathbf{C}_r & = & \\pmatrix{ \\boldsymbol{c}_1 & \\boldsymbol{c}_2 & \\dots & \\boldsymbol{c}_r }                                                                  & \\text{# Reduced basis} \\\\\n",
    "{\\tiny 6.}            & \\mathbf{A}   & = & \\left\\{ \\boldsymbol{a}_i | \\boldsymbol{a}_i = \\mathbf{C}_r^{\\top}\\mathbf{K}_i ~ \\text{ for } i = 1, \\dots, n \\right\\}  \\quad             & \\text{# Projected data} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ec0c1644ccd3a57785914a4e8417b7d",
     "grade": false,
     "grade_id": "cell-910b7b010175a110",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here, $\\mathbf{D}$ is the $n \\times d$ dataset (or \"design\") matrix ($n$ points of $d$ dimensions), $K$ is a kernel function and $r$ is the size of the basis we want to compute.\n",
    "\n",
    "$\\mathbf{I}$ is the $n \\times n$ identity matrix\n",
    "and $\\mathbf{1}_{n \\times n}$ is the $n \\times n$ matrix filled with 1s ([numpy.ones(shape=(n, n))](https://numpy.org/doc/stable/reference/generated/numpy.ones.html?highlight=ones#numpy.ones) in Python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9838ee9eeb3e1f898bcbb865ff8b696",
     "grade": false,
     "grade_id": "cell-2c4d646b813f927a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Note**: Eigenvectors and eigenvalues can be computed with [numpy.linalg.eig(K)](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html#numpy.linalg.eig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5216c49e883c21fe999568e24521aecb",
     "grade": false,
     "grade_id": "cell-613885c3d354740a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def kernel_pca(data: np.ndarray, kernel_function, basis: int):\n",
    "    \"\"\"\n",
    "    Kernel PCA algorithm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        the n-by-d matrix of inputs (n points of d dimensions - D in the equations above)\n",
    "    kernel_function : function\n",
    "        the kernel function used to make the kernel matrix of inputs (K in the equations above)\n",
    "    basis : int\n",
    "        the number of basis to compute (r in the equations above)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    projected_data : numpy.ndarray\n",
    "        a 1D vector of labels of length n (e.g. c[i] = C_j means \"x_i is belongs to cluster C_j\")\n",
    "    \"\"\"\n",
    "\n",
    "    n = data.shape[0]   # number of  points\n",
    "\n",
    "    # Compute the kernel matrix\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Center the kernel matrix\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Reduced basis\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "   \n",
    "    # Projected data\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return projected_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35da4d67f4ba164381c60899f24f97eb",
     "grade": false,
     "grade_id": "cell-05a60433bb6661a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2**: test the implementation on the following dataset (using the quadratic kernel defined in Part 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9df5f9f21c7e06926c58fee877f6f8dc",
     "grade": false,
     "grade_id": "cell-1bfd36cee65420d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "data = np.random.multivariate_normal(mean=np.zeros(2), cov=np.array([[1, 0], [0, 1]]), size=100)\n",
    "data[:,0] = 0.2 * data[:,0]**2 + data[:,1]**2 + 0.1 * data[:,0] * data[:,1]\n",
    "plt.scatter(data[:,0], data[:,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "362ab3ed2e89478b94e97af8eeadaf91",
     "grade": true,
     "grade_id": "cell-9d4fd361878ad6b0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "kernel = PolynomialKernel()\n",
    "projected_data = kernel_pca(data, kernel, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57e0b103999a44b18ded6d7fea05dcae",
     "grade": false,
     "grade_id": "cell-f1d7d00addbeba22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(projected_data[:, 0], projected_data[:, 1])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11c43aff60b021effed2ad04af7b118f",
     "grade": false,
     "grade_id": "cell-92cb3118f8fbcc93",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 4: Kernel Ridge Regression (Bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d718a8596e374b5a6766a2b167eb3ad4",
     "grade": false,
     "grade_id": "cell-a9ff4ded67cc5806",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 1**: implement in Python the Kernel Ridge Regression algorithm defined in [the lecture notes](https://moodle.polytechnique.fr/pluginfile.php/203685/mod_resource/content/1/Notes_12.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3127edae86ce32e6c0aa5e770e1213d",
     "grade": false,
     "grade_id": "cell-4dac02454a1cdb4e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def kernel_ridge_regression(x: float, dataset: np.ndarray, kernel_function, lambda_: float):\n",
    "    \"\"\"\n",
    "    Kernel ridge regression\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        the value to predict\n",
    "    dataset : ndarray\n",
    "        the dataset used to make the kernel matrix of inputs\n",
    "    kernel_function : function\n",
    "        the kernel function used to make the kernel matrix of inputs\n",
    "    lambda_ : float\n",
    "        the regularization strength (lambda is a reserved python keyword)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    y_pred : float\n",
    "        the predicted value for x\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aeb53241cc9256c4445944beee93d2ed",
     "grade": false,
     "grade_id": "cell-739828482caedd40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2**: test the implementation on the following non linear 1D dataset used in lab 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ea36b64cacea39f236f95abb30a3168",
     "grade": false,
     "grade_id": "cell-155036a385cc3d0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame([[2., 0.],\n",
    "                        [5., 2.],\n",
    "                        [7., 1.],\n",
    "                        [10., 2.],\n",
    "                        [14., 4.],\n",
    "                        [16., 3.],\n",
    "                        [17., 0.]], columns=['x', 'y'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11e8bc2dbc8809a5e5d6788947f03b7d",
     "grade": false,
     "grade_id": "cell-fca4665c50332dd7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "kernel_function = PolynomialKernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0bf8c7adc808ec68a2422d623aeea7e",
     "grade": true,
     "grade_id": "cell-45b6b41f5f20fcce",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_pred = np.linspace(0., 20., 200)\n",
    "y_pred = [kernel_ridge_regression(x, dataset, kernel_function, lambda_=6.) for x in x_pred]\n",
    "\n",
    "ax = dataset.plot.scatter(x='x', y='y', label=\"Dataset\", figsize=(12,8))\n",
    "ax.plot(x_pred, y_pred, \"-r\", label=\"Kernel ridge regression\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSE204",
   "language": "python",
   "name": "cse204"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
