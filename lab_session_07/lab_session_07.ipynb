{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "5YuK2f4u_CzI",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "395900a575379bfc34c7d5f650110b76",
     "grade": false,
     "grade_id": "cell-d65e9154014fb110",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CSE204 - Introduction to Machine Learning - Lab Session 7: Building a Neural Network From Scratch - Backpropagation\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/adimajo/CSE204-2021/master/data/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSE204-2021](https://moodle.polytechnique.fr/course/view.php?id=12838) Lab session #07\n",
    "\n",
    "J.B. Scoggins - Adrien Ehrhardt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb10c054e8d9d9930bd5f2cffef6e096",
     "grade": false,
     "grade_id": "cell-5aa6abed17fe1b9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This lab is a direct follow-up of Lab session #06 on Neural networks. We will now make the network trainable and... train it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b68684f19421c3319875c9c6b70e1914",
     "grade": false,
     "grade_id": "cell-f77480ed47c9c6c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Recall last lab's notations\n",
    "\n",
    "We will consider simple feed-forward networks that can be described by the following recursive relationship,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&z^l = a^{l-1} W^l + b^l,\\\\\n",
    "&a^l = \\sigma^l(z^l),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $a^l$ is the output (activation) of layer $l$ which is a nonlinear function $\\sigma^l$ of a linear transformation of the previous layer's output.  The linear transformation is performed using the weight matrix $W^l$ and bias vector $b^l$ associated with the layer $l$.  We will denote the last layer in the network with a capital $L$ superscript.  The recursion is stopped by setting $a^0 = x$, where $x$ is the input vector to our network.  Note that in the recursive expressions above, we implicitly assume that our input/output vectors are row vectors.  The reason for this will be evident later.\n",
    "\n",
    "Taking this notation into account, we see that a network with $L-1$ hidden layers is fully expressed by its $L$ weight matrices, bias vectors, and activation functions.  We can denote the set of trainable parameters in our network by $\\theta = \\{ W^1, \\dots, W^L, b^1, \\dots, b^L \\}$.\n",
    "\n",
    "In this lab, we are only concerned with supervised learning tasks.  Recall that in supervised learning, we have a dataset represented by a list of $(x, y)$ pairs where $x$ is the input to our model and $y$ is the desired output.\n",
    "\n",
    "- For regression problems where we want to fit a function $y = f(x)$, $x$ is the independent variable vector, and $y$ is the function value.\n",
    "- In classification problems, $x$ will correspond to a set of attributes and $y$ the corresponding label.\n",
    "\n",
    "The goal of supervised learning is to \"train\" our network by adjusting its parameters in order to minimize a cost function over the entire training set,\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\mathcal{L} = \\min_\\theta \\sum_{i=1}^N \\ell^{(i)}\n",
    "$$\n",
    "\n",
    "where $\\ell^{(i)} = \\ell(\\sigma^L(\\sigma^{L-1}(\\dots(x^{(i)}W^1 + b^1)\\dots)W^L + b^L), y^{(i)})$ and $\\ell(\\hat{y}, y)$ denotes the particular form of the loss function being considered.  In this lab, we will use 2 different loss functions:\n",
    "\n",
    "1. Quadratic Loss: $\\ell(\\hat{y}, y) = \\|\\hat{y} - y\\|^2$\n",
    "2. Cross-entropy Loss: $\\ell(\\hat{y}, y) = -[ y \\ln\\hat{y} + (1-y)\\ln(1-\\hat{y})]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71775ce9c06d538191d677a8c5828b59",
     "grade": false,
     "grade_id": "cell-6ebb9d1d9c52b727",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff3405e65ef0ba1b01cfe6e943ca02a4",
     "grade": false,
     "grade_id": "cell-09690574e9c0b18b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This might take some time and output warnings: don't worry.\n",
    "# This might error, e.g. if you didn't install the CSE204 kernel.\n",
    "# You can try !pip install tensorflow in a new cell to install it on the fly\n",
    "# but be sure the remove the direcctive before uploading your work!\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e745a56a2e2490e2fc33cf3113840df",
     "grade": false,
     "grade_id": "cell-e0ce032d0410b348",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here are the activations classes you (hopefully) managed to implement last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85cef38875383cbf5e3b9e4cf6920146",
     "grade": false,
     "grade_id": "cell-2e649226bc023e51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    \"\"\"\n",
    "    Base activation node class\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(self, x: float) -> float:\n",
    "        \"\"\"\n",
    "        What to do when instantiating an object: this basically says Activation(x)\n",
    "        will be equivalent to Activation()(x).\n",
    "\n",
    "        :param float or numpy.array x: input of the activation\n",
    "        :return: call the activation function\n",
    "        :rtype: float or numpy.array\n",
    "        \"\"\"\n",
    "        return self.__call__(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e74864f3c476f0df8292c26d20b45abf",
     "grade": false,
     "grade_id": "cell-3ee859296687382e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Identity(Activation):\n",
    "    \"\"\"\n",
    "    Identity activation node\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def __call__(x: float) -> float:\n",
    "        \"\"\"\n",
    "        Implements the identity activation function\n",
    "\n",
    "        :param float or np.array x: input of the activation\n",
    "        :return: identity of input\n",
    "        :rtype: float or np.array\n",
    "        \"\"\"\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(x: float) -> float:\n",
    "        \"\"\"\n",
    "        Implements the derivative of the identity activation function\n",
    "\n",
    "        :param float or numpy.array: input of the activation\n",
    "        :return: derivative of input\n",
    "        :rtype: float or numpy.array\n",
    "        \"\"\"\n",
    "        return np.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38be63aeab49081452651e90d5e2a301",
     "grade": false,
     "grade_id": "cell-08e8b8388a837157",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Threshold(Activation):\n",
    "    \"\"\"\n",
    "    Threshold activation node\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def __call__(x: float) -> float:\n",
    "        \"\"\"\n",
    "        Implements the threshold activation function\n",
    "\n",
    "        :param float or numpy.array x: input of the activation\n",
    "        :return: if x > 0, 1, else 0\n",
    "        :rtype: float or numpy.array\n",
    "        \"\"\"\n",
    "        return np.where(x > 0.0, 1.0, 0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(x: float) -> float:\n",
    "        \"\"\"\n",
    "        Implements the derivative of the threshold activation function\n",
    "\n",
    "        :param float or numpy.array: input of the activation\n",
    "        :return: derivative of input\n",
    "        :rtype: float or numpy.array\n",
    "        \"\"\"\n",
    "        return np.zeros_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9967eced7996f6fae1ad5d6ac3b23d7f",
     "grade": false,
     "grade_id": "cell-bf37f3ffb0cd15a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    \"\"\"\n",
    "    Sigmoid activation node\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def __call__(x: float) -> float:\n",
    "        \"\"\"\n",
    "        Implements the sigmoid activation function\n",
    "\n",
    "        :param float or numpy.array x: input of the activation\n",
    "        :return: sigmoid transform of input\n",
    "        :rtype: float or numpy.array\n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(x: float) -> float:\n",
    "        \"\"\"\n",
    "        Implements the derivative of the sigmoid activation function\n",
    "        (you can make good use of Sigmoid(...)!)\n",
    "\n",
    "        :param float or numpy.array: input of the activation\n",
    "        :return: derivative of input\n",
    "        :rtype: float or numpy.array\n",
    "        \"\"\"\n",
    "        sigmoid = Sigmoid(x)\n",
    "        return sigmoid * (1.0 - sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e83d1618c5d95324f500c21b702d9e8",
     "grade": false,
     "grade_id": "cell-4cda9f4baec709e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here is the `Network` class you (hopefully) managed to implement last week.\n",
    "**Notice that the implementation and signature of the `feed_forward` method changed so as not to recompute things in `back_propagation`** (see Question 1.3).\n",
    "\n",
    "A `loss` attribute has also been added to record the evolution of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "724a95fd1fbd184e9173df065d17289c",
     "grade": false,
     "grade_id": "cell-aab6d3b791876823",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"\n",
    "    A simple implementation of a feed-forward artificial neural network.\n",
    "    Work on this class throughout the entire exercise, rerunning this cell after each update.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sizes: list, activations: list):\n",
    "        \"\"\"\n",
    "        Construct a network given a list of the number of neurons in each layer and\n",
    "        a list of activations which will be applied to each layer.\n",
    "\n",
    "        :param list sizes: A list of integers representing the number of nodes in each layer,\n",
    "        including the input and output layers.\n",
    "        :param list activations: A list of callable objects representing the activation functions.\n",
    "        Its size should be one less than sizes.\n",
    "        \"\"\"\n",
    "        self.sizes = sizes\n",
    "        self.sigmas = activations\n",
    "        self.loss = []\n",
    "\n",
    "        self.biases = [np.random.randn(1, n) for n in sizes[1:]]\n",
    "        self.weights = [\n",
    "            np.random.randn(m, n) for m, n in zip(sizes[:-1], sizes[1:])\n",
    "        ]\n",
    "        # Optional: better initialization\n",
    "        # self.weights = [\n",
    "        #     np.random.randn(m, n) / np.sqrt(m) for m, n in zip(sizes[:-1], sizes[1:])\n",
    "        # ]\n",
    "\n",
    "    def num_params(self) -> int:\n",
    "        \"\"\"Returns the total number of trainable parameters in this network.\"\"\"\n",
    "        return sum([m * n + n for m, n in zip(self.sizes[:-1], self.sizes[1:])])\n",
    "\n",
    "    def feed_forward(self, x: np.array):\n",
    "        \"\"\"\n",
    "        Evaluates the network for the given input and returns the intermediary results.\n",
    "\n",
    "        :param numpy.array x: A numpy 2D-array where the columns represent input variables\n",
    "        and rows represent independent samples.\n",
    "        :return: output of the network\n",
    "        :rtype: np.array, list, list\n",
    "        \"\"\"\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "\n",
    "        # Forward pass, storing z and a as we go\n",
    "        for W, b, sigma in zip(self.weights, self.biases, self.sigmas):\n",
    "            zs.append(np.matmul(activations[-1], W) + b)\n",
    "            activations.append(sigma(zs[-1]))\n",
    "        return activations[-1], activations, zs\n",
    "\n",
    "    def back_propagation(self, x: np.array, y: np.array, cost, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Compute gradients of cost function w.r.t network parameters for a single training point.\n",
    "\n",
    "        :param numpy.array x: a single input in a 2D array (1, #features)\n",
    "        :param numpy.array y: a single output\n",
    "        :param Cost cost: a cost class\n",
    "        :param bool verbose: whether to print debugging information (you don't have to implement this feature)\n",
    "        :return: list of gradients of weights, list of gradients of biases\n",
    "        :rtype: list, list\n",
    "        \"\"\"\n",
    "        # Exercise 1.3\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return w_grad, b_grad\n",
    "\n",
    "    def update_step(self, x_train: np.array, y_train: np.array, cost, learning_rate: float,\n",
    "                    verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Compute the average parameter gradients over the whole training set and do a GD step.\n",
    "        N.B.: this does not return anything!\n",
    "\n",
    "        :param numpy.array x_train: A design matrix (2D array (#samples, #features))\n",
    "        :param numpy.array y_train: A vector / array of responses\n",
    "        :param Cost cost: A callable object with signature cost(y_pred, y) for computing the cost of a single\n",
    "        training example.\n",
    "        :param float learning_rate: The learning rate to use in the GD step.\n",
    "        :param bool update_step: Perform an update step: average all the gradients and update the parameters.\n",
    "        \"\"\"\n",
    "        # Storage room for sum of training samples gradients\n",
    "        w_grad_sum = [np.zeros_like(w) for w in self.weights]\n",
    "        b_grad_sum = [np.zeros_like(b) for b in self.biases]\n",
    "        # Exercise 1.4\n",
    "        ## Suggestions\n",
    "        # Compute gradients of weights and biases for all layers and samples\n",
    "        # Sum gradients for all samples\n",
    "        # Update the weights and biases\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def train(self, x_train, y_train, cost, learning_rate: float, epochs: int = 100, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Trains this network on the training data, i.e. iterate the update step.\n",
    "        N.B.: this does not return anything!\n",
    "\n",
    "        :param np.array x_train: A design matrix\n",
    "        :param np.array y_train: A vector of responses\n",
    "        :param Cost cost: A callable object with signature cost(y_pred, y) for computing the cost of a single\n",
    "        training example.\n",
    "        :param float learning_rate: The learning rate to use in the GD step.\n",
    "        :param int epochs: The number of epochs to train with.\n",
    "        :param bool verbose: Whether to print the loss at each epoch.\n",
    "        \"\"\"\n",
    "        for i in range(epochs):\n",
    "            self.update_step(x_train, y_train, cost, learning_rate)\n",
    "\n",
    "            # Compute the loss\n",
    "            y_pred, _, _ = self.feed_forward(x_train)\n",
    "            loss = cost(y_pred, y_train)\n",
    "            self.loss.append(loss)\n",
    "            if verbose:\n",
    "                print('epoch:', i, '\\n\\tloss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "CcgyCRvg_Czs",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2182e6339bd2f55412960863b1cb75ad",
     "grade": false,
     "grade_id": "cell-d27916f41d987521",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1: Implement the training algorithm\n",
    "\n",
    "Before we can implement the training algorithm, we need to define a cost function. For now, let's create a base class `Cost`, similar to the base class `Activation`, for simplicity, and a `QuadraticCost` class representing the quadratic cost which was given in the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a183091380ba98c283b5782a16ffb2ed",
     "grade": false,
     "grade_id": "cell-6304d680c7702e4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Cost:\n",
    "    def __new__(self, y_pred: np.array, y: np.array) -> float:\n",
    "        return self.__call__(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08525fb098de311a8ad693474f263e76",
     "grade": false,
     "grade_id": "cell-ad9ec66add87f74e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. Implement the quadratic cost defined above in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "Dq7HxJY1_Czu",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a38a54c530444b20db0c9b0a0d18d41",
     "grade": false,
     "grade_id": "cell-06e268e99d5f4984",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class QuadraticCost(Cost):\n",
    "    @staticmethod\n",
    "    def __call__(y_pred: np.array, y: np.array) -> float:\n",
    "        \"\"\"\n",
    "        Computes the quadratic cost function.\n",
    "\n",
    "        :param numpy.array y_pred: y_hat for each sample\n",
    "        :param numpy.array y: y for each sample\n",
    "        :return: Squared L2 norm of the difference\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(y_pred: np.array, y: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Computes the derivative of the quadratic cost function.\n",
    "\n",
    "        :param numpy.array y_pred: y_hat for each sample\n",
    "        :param numpy.array y: y for each sample\n",
    "        :return: derivative of cost function w.r.t. y_pred\n",
    "        :rtype: numpy.array\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "214d15a23561b721121fbe304d8a37b1",
     "grade": true,
     "grade_id": "cell-98a36c947d6f4f78",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert QuadraticCost(0, 0) == 0\n",
    "assert QuadraticCost.derivative(0, 0) == 0\n",
    "assert QuadraticCost(np.array([0, 0]), np.array([0, 0])) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "sVItBWoq_Czw",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ae358dac8655593b794c48f09370b76",
     "grade": false,
     "grade_id": "cell-198d1a661f9903e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Implement the `back_propagation` function in the `Network` class.\n",
    "\n",
    "With this cost defined, let's implement three remaining functions in `Network` that will allow us to train our models. The first is the `back_propagation` method which takes a single training sample $x, y$ and a `Cost` class  like the one above, and computes the gradient of the cost function w.r.t. to all the weights and biases using the backpropagation algorithm. Use the formulas found [at the end of lab #06](https://adimajo.github.io/CSE204-2021/lab_session_06/lab_session_06.html). It sould return the lists of $\\nabla_{W^l}\\ell_p$ and $\\nabla_{b^l}\\ell_p$ for $1 \\leq l \\leq L$.\n",
    "\n",
    "**Remember to execute the `Network` class again before running the tests below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b78861a079557480a9f8c371a0b1462",
     "grade": true,
     "grade_id": "cell-823828240e0e5860",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a single training point\n",
    "x = np.array([1, 2]).reshape(1, 2)\n",
    "y = np.array([0])\n",
    "\n",
    "# Define a network\n",
    "network = Network([2, 5, 1], [Sigmoid, Identity])\n",
    "\n",
    "# Perform back-propagation\n",
    "w_grad, b_grad = network.back_propagation(x, y, QuadraticCost)\n",
    "\n",
    "assert len(w_grad) == 2  # 2 layers (1 hidden, 1 output)\n",
    "assert len(b_grad) == 2  # 2 layers (1 hidden, 1 output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1dbf4b67b4ba02687ba5601c1a837a77",
     "grade": true,
     "grade_id": "cell-7d0800d59f3e73f5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2007b69d633fa72d5c64be4de01ebef5",
     "grade": false,
     "grade_id": "cell-27b35f458071a5ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Implement the `update_step` function in the `Network` class.\n",
    "\n",
    "Next, implement a single epoch of training by **averaging** the gradients over the whole dataset (calling `back_propagation` on each data point) and using the GD update rule to update the weights and biases.\n",
    "\n",
    "The `update_step` method is used by the `train` method successively in a loop as you can see in the `Network` class implementation above.  Assuming you have correctly implemented the `backprop` and `update_step` functions correctly, you should now be able to train the network on real training data.\n",
    "\n",
    "**Remember to execute the `Network` class again before running the tests below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a50229be52bec50fcd7ca2d789061a7b",
     "grade": true,
     "grade_id": "cell-f62a429b993c10b6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a single training point\n",
    "x = np.random.uniform(-1, 1, (100, 1))\n",
    "y = np.sin(np.pi * x)\n",
    "\n",
    "# Define a network\n",
    "network = Network([1, 100, 1], [Sigmoid, Identity])\n",
    "\n",
    "# Store weights and biases before the update to see if something changed\n",
    "weights_before = network.weights[0].copy()\n",
    "biases_before = network.biases[0].copy()\n",
    "\n",
    "\n",
    "# Perform an update step\n",
    "network.update_step(x, y, QuadraticCost, 0.1)\n",
    "\n",
    "# Assert that something did change\n",
    "assert np.any(np.not_equal(network.weights[0], weights_before))\n",
    "assert np.any(np.not_equal(network.biases[0], biases_before))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "HOoISY4e_Czy",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b48739411395b9aa1b0c2e7c4ddf06c",
     "grade": false,
     "grade_id": "cell-85bd25f54fbff113",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2: Test training on simple regression problem.\n",
    "\n",
    "Run the following code to test the network.  If all is correct, you should see the total loss dropping and the network will learn the sine function.  \n",
    "\n",
    "1. Play with different hyperparameters of the network and training such as the number of layers, number of nodes per layer, training rate, and number of training samples. Can you improve the training performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trHvjm86_Czy"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters to tune\n",
    "n_iterations = 10000\n",
    "lr = 0.15\n",
    "verbose = False\n",
    "sizes = [1, 3, 3, 1]\n",
    "activations = [Sigmoid, Sigmoid, Identity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79d88daa5674b24b67c379c9ffa2e429",
     "grade": false,
     "grade_id": "cell-dbc8b871cbe83a32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Training the network\n",
    "np.random.seed(1)\n",
    "network = Network(sizes, activations)\n",
    "\n",
    "network.train(x_train=x, y_train=y, cost=QuadraticCost,\n",
    "              learning_rate=lr, epochs=n_iterations, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29664a0e38bb73b6a084bf72d31e729a",
     "grade": false,
     "grade_id": "cell-b98d7af73e35e239",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Some test samples\n",
    "x_pred = np.arange(-1, 1, 0.01).reshape(200, 1)\n",
    "y_pred, _, _ = network.feed_forward(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e240bda4f2797f64063c9a2611cbc3a",
     "grade": false,
     "grade_id": "cell-c235079a0acb1424",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot train samples, truth, test samples and predictions\n",
    "plt.plot(x, y, 'k.', label=\"Truth\")\n",
    "plt.plot(x_pred, y_pred, 'k-', label=\"Prediction\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(range(n_iterations), network.loss, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(range(int(n_iterations / 2), n_iterations),\n",
    "         network.loss[int(n_iterations / 2):],\n",
    "         label=f\"Training loss, last {int(n_iterations / 2)} observations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79cf620c6dadf85b381811f6fd95652f",
     "grade": true,
     "grade_id": "cell-d80d44f5e291891e",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "oS8FykEW_Cz1",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e4e49747225ed587e6db21fb10e8b8a",
     "grade": false,
     "grade_id": "cell-2755c7b7a760993b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 3: Fashion items classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "T3z-qi2D_Cz2",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5210a8369be01ce141d105cbf6723de",
     "grade": false,
     "grade_id": "cell-893e161ea291f57e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's test our model on a more sophisticated dataset, namely [Zalando research's fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist). This dataset includes 70,000 images of fashion items 28x28 pixels (=784), split into a training set of 60,000 images and a test set of 10,000 images. Each image is labeled with a digit from 0 to 9.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e004948b7508dc411e043e19eb11088d",
     "grade": false,
     "grade_id": "cell-44db52d7a02aa251",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The original [MNIST dataset](http://yann.lecun.com/exdb/mnist/) consists in handwritten digits, e.g. to automatically read the addresses on mails. The performance of neural networks on this task was so much better than previous approaches that it (re-)sparked a lot of interest in them (and brought fame to Yann LeCun). It is now considered an \"easy\" task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "DMPGw1_2_Cz3",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "facc736bf7bdca6d2824f8af1119c154",
     "grade": false,
     "grade_id": "cell-3889c95ba020ab46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3: Get to know the dataset\n",
    "\n",
    "1. Just run the following cell to download the MNIST database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dc17cba28df8cd606d9b76a423595cd",
     "grade": false,
     "grade_id": "cell-d5b3ae3502947997",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This might take some time to download the datasets.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "DOGb47wm_Cz7",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22e31dd64c164ac821c55a064eb8cd7c",
     "grade": false,
     "grade_id": "cell-655bc4b54e585eef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Next check that the shapes of the training and test data, and their content, are what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "Hx6Yqm80_Cz7",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28728e8b6b4f20db51a8eb819240dad4",
     "grade": false,
     "grade_id": "cell-67f8be678bb72756",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fcf538c58eb4a99d2a038cdf790c1a97",
     "grade": true,
     "grade_id": "cell-6842148492190a1a",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "lmhztu8w_Cz-",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1683b39bd33bc257f08713409b2c7ca",
     "grade": false,
     "grade_id": "cell-fb976918d0bc982f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Get a feel for the dataset by displaying some images.\n",
    "\n",
    "*Python hint*: [plt.imshow](https://matplotlib.org/3.5.1/api/_as_gen/matplotlib.pyplot.imshow.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "ecSEcD82_Cz-",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2af21fd22bdc3e952e2f97f15a3504a",
     "grade": true,
     "grade_id": "cell-d83572104abecf8d",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "jycvYjC8_C0A",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0f30e2e8b4e851d149ae72d2689e091",
     "grade": false,
     "grade_id": "cell-102eb0e8b1b6d593",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 4: Create a one-hot encoding of the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "07iAfT3F_C0B",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6958f0a74b625623f853017f5a4dc1ab",
     "grade": false,
     "grade_id": "cell-22f82d6261e58de6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "`y_train` and `y_test` currently contain the digits 0-9, which is not suited for training as we want to perform a 10-class classification training on our network. To fix this, we need to perform a so-called one-hot encoding on the labels which convert each digit into a vector of length 10 full of zeros, except the element corresponding to the digit which should be 1.  For example, if our label is 3, then the corresponding one-hot encoding is [0, 0, 0, 1, 0, 0, 0, 0, 0, 0].\n",
    "\n",
    "1. Convert the `y_train` and `y_test` vectors into one-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "Cbji5mMI_C0D",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9978b9717628783e3a7f07f9f21970b",
     "grade": false,
     "grade_id": "cell-f3542df3b7808c77",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# y_train =  ...\n",
    "# y_test =  ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43c7f99a4a986a8c3b18980e0f2aa0fb",
     "grade": true,
     "grade_id": "cell-f71e9a13bf89af5e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "2m0XRWBV_C0G",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85d2bc63790ebd9cdeee736699908d3e",
     "grade": false,
     "grade_id": "cell-6643c0869bd1da0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 5: Train your network to recognize fashion items\n",
    "\n",
    "1. Implement the cross-entropy loss function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "m0g-NQxS_C0I",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3755f9cfe906cfb335bfe0e13ebf639f",
     "grade": false,
     "grade_id": "cell-44314e995a469cda",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class CrossEntropy(Cost):\n",
    "    \"\"\"Cross Entropy loss function\"\"\"\n",
    "    @staticmethod\n",
    "    def __call__(y_pred: np.array, y: np.array) -> float:\n",
    "        \"\"\"\n",
    "        Value of the cross entropy loss given predictions and true labels\n",
    "\n",
    "        :param numpy.array y_pred: 1D array associated with the predicted probability of y=1\n",
    "        :param numpy.array y: 1D array of true (0/1) classes\n",
    "        :return: value of the loss\n",
    "        :rtype: float or numpy.array\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative(y_pred: np.array, y: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Derivative of the cross entropy loss function\n",
    "\n",
    "        :param numpy.array y_pred: 1D array associated with the predicted probability of y=1\n",
    "        :param numpy.array y: 1D array of true (0/1) classes\n",
    "        :return: gradient of the loss w.r.t. y_pred\n",
    "        :rtype: numpy.array\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2faf9dd4da54e661a1400e24fabd0ba",
     "grade": true,
     "grade_id": "cell-fe1ca1fd8adacb35",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert CrossEntropy(0.5, 0) == - np.log(0.5)\n",
    "assert CrossEntropy.derivative(0.5, 0) == 2.0\n",
    "assert int(CrossEntropy(np.array([0.5, 0.5]),\n",
    "                    np.array([0, 1]))) == 1\n",
    "assert CrossEntropy.derivative(0.5, 0) == 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "YIgJRtRj_C0M",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77b1c01e10c9bd7fc1d90240ced7699b",
     "grade": false,
     "grade_id": "cell-bbf461afaed81e3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Instanciate a network with 1 hidden layer of 30 nodes, with sigmoid activations and a cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "-piflCMb_C0N",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e26a60c26059050853c69a8c1cef0546",
     "grade": false,
     "grade_id": "cell-56ed8b574c919936",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# network = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2269fa7cf5e0fd3384bc9632f42a611",
     "grade": true,
     "grade_id": "cell-e43aeee993a1f613",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "edd4d5f88effff137ec9d9dbcae36358",
     "grade": false,
     "grade_id": "cell-906f3384373591aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Train the network using the training set, any learning rate / number of iterations (see 4.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "b9uTukZR_C0P",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff6ac8bbea70adeb6012b3a5bcbb2358",
     "grade": false,
     "grade_id": "cell-b468730447b2a263",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train the network for X epochs with a learning rate of Y\n",
    "# Our implementation does not deal with 3D inputs, so you need\n",
    "# to \"flatten\" the 28x28 images in a single vector / observation / sample / row.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3cae3bea5f571df5085a1e083801e257",
     "grade": true,
     "grade_id": "cell-343d1f66425feec6",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f0d9a4aa7c57f0d1922d7e1a8799ef0",
     "grade": true,
     "grade_id": "cell-c979953ac5fa386c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bdcd4cb7ddfb5b9ad7c0035e6ccb26e",
     "grade": true,
     "grade_id": "cell-23e62b07c7bdf3df",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e339bf01c8b388d6ef689f7567ef14e4",
     "grade": false,
     "grade_id": "cell-f070fe33fd442069",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "4. Plot the evolution of the loss and go back to training, tweaking the learning rate / number of iterations until the plot looks satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20a93c8a613f828b232f5ccce4907535",
     "grade": true,
     "grade_id": "cell-ac5b89a05c004f79",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "2fwDAYxw_C0T",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb1d64375cc8813e90c020fe749cc8b7",
     "grade": false,
     "grade_id": "cell-5746671ea9e7d9ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Compute the accuracy of the model on the test data and compare a few predictions to their images (see labels [here](https://github.com/zalandoresearch/fashion-mnist#labels))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "6T0gWj6M_C0T",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2b8a8ccb5abfc8c6d832a40be48054a",
     "grade": true,
     "grade_id": "cell-7c906940128eb844",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the accuracy\n",
    "# accuracy = ...\n",
    "\n",
    "# For example, display an example of a class probability and its true value\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48803a834b74e289f708804cf56fcd35",
     "grade": true,
     "grade_id": "cell-08097250a35a932d",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "igiC3hoK8Sml",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7bb743d70056a6cb039381a09fe7767",
     "grade": false,
     "grade_id": "cell-5271b4893d50b9fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Extra\n",
    "\n",
    "At this point you have a fully functioning neural network model which can be used in real machine-learning problems.  There are many things which you could add at this point to make the model more effective.  Consider playing with some of the following:\n",
    "\n",
    "- Adding weight regularization\n",
    "- Using different activation functions such as ReLU or Tanh\n",
    "- Improve the efficiency by making the backprop algorithm work on whole batches at once\n",
    "- Implement the SGD algorithm\n",
    "- Redesign the model to allow plug-and-play layers to be stacked together, allowing more complicated layers such as convolutions to be used.\n",
    "\n",
    "Of course the list is endless, just have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "ajeTludS8Smm",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74bbe0be1509b6c6a769ae22b71af3fa",
     "grade": false,
     "grade_id": "cell-0455e8c24e6650a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Next Step: Using Tensorflow\n",
    "\n",
    "As you have now seen, building a neural network and training from scratch can require a significant amount of effort.  Furthermore, the naive design of our `Network` class does not allow us to easily incorporate new kinds of layers such as convolutions, dropout, or batch normalization.  In practice of course, several libraries already exist that provide all of this functionality for you, so that you can focus on building a model for your specific dataset.  Here, we will show you how to redo the digit classification example above, with less than 10 lines of code using the Tensorflow library. You can [find this example](https://www.tensorflow.org/tutorials) directly on the Tensorflow webiste if you are interested.  Let's walk through the code below.\n",
    "\n",
    "### Import Tensorflow and download the MNIST dataset\n",
    "\n",
    "Tensorflow includes a high-level neural network architecture building API called Keras which makes it easy to create neural network models using \"off-the-shelf\" layers.  In addition, Keras also provides commonly used datasets, such as the MNIST dataset.  In the first line below, Tensorflow is imported.  Second, the `keras.datasets` API is used to dowload the full dataset into two groups, a training set and a testing set, like you have seen before.  In the third line, the pixel data (integers between 0-255) is converted to real values between 0.0-1.0, making it easier for our NN model to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "h44cp_s18Smo",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ab1e00479e3db5ef3373dae539c244f",
     "grade": false,
     "grade_id": "cell-9bccc8c26401d948",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Build the NN model\n",
    "\n",
    "In the next two lines, the Keras API is used to create a simple NN model with the following layers:\n",
    "\n",
    "- `Flatten` takes an N-dimensional array and reshapes it into a 1D array. In this case, the images are 28x28 pixels, which get converted to a 784 1D tensor.\n",
    "- `Dense` is exactly like the type of implicit layers we coded in our `Network` class.  That is, they take some 1D input and apply a linear transformation before applying a nonlinear activation.  In this case we are using 512 nodes in the dense layer.\n",
    "- `Dropout` is a little more advanced.  As you have seen in the lecture, a dropout layer sets some percentage of the inputs to the layer to zero.  During training, each time data is propagated through the dropout layer, a different random set of nodes is zeroed out.  This forces the network to make more generalizations about the data and reduces overfitting.  When training is done, the dropout layer allows all the nodes to pass through, but multiplies the result by the dropout factor such that the sum remains the same.  In this case, 20% of the nodes are randomly canceled out during training and the result during evaluation is multiplied by 0.8.  \n",
    "- The final layer is a dense layer of 10 nodes corresponding to the one-hot encoding of the digit labels.  Note that a softmax activation function is used so that these nodes will represent propbabilities.\n",
    "\n",
    "In the last line, the model is \"compiled\" with an optimizer (Adam), loss function (sparse categorical crossentropy), and a metrics, which is responsible for providing useful output during training.  In this case, the `accuracy` metric is used which computes the total number of training samples that were correctly categorized as trianing progresses.\n",
    "\n",
    "You may modify the model as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fj-WRBsu8Smp"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "EwMXv8T_8Sms",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dccfe80cf80907ee9233874804851ddc",
     "grade": false,
     "grade_id": "cell-e83e0c52ff8080e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Train and evaluate the model\n",
    "\n",
    "In the last step, we train the model.  This happens in the first line below, were we call the model's `fit` function, which takes the training data and a number of epochs.  The number of batches per epoch is handled by default.  In the second line, we evaluate the model on our testing data to see how well it does on data it has never seen before.  Note that we get a ~ 79% accuracy with this simple model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "RVEPKwjU8Sms",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37a488f2a6c6ac18ae8c8100478f288a",
     "grade": false,
     "grade_id": "cell-09eaf4cfaec4ab83",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=n_epoch)\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print(\"Final test loss:\", loss)\n",
    "print(\"Final test accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2371caa0b28b913376e8306bcd5666db",
     "grade": false,
     "grade_id": "cell-de90e01fe6b77e9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"The probabilities for each class for the first test point:\\n\", model.predict(x_test)[0, :].reshape(-1, 1))\n",
    "print(\"The true class of the first test point:\\n\", y_test[0].reshape(-1, 1))\n",
    "plt.imshow(x_test[0, :]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47e9636c74daf4b3a7190c1ff01fc2e8",
     "grade": false,
     "grade_id": "cell-a4c11d593bc96741",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's do the same with PyTorch. There are many deep learning frameworks, but Tensorflow and PyTorch are the most used, with Tensorflow becoming out-of-fashion (!) in favor of PyTorch, generally dubbed \"simpler\" (well, we'll see how that turns out!).\n",
    "\n",
    "(Tensorflow is indeed more complicated but in the code above, we relied on Keras, a library built on top and meant to simplify the usage of several deep learning frameworks.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ab7e87d5f4971f8a8ab763a9199bbf7",
     "grade": false,
     "grade_id": "cell-7eafa02b261d4f8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We did **not** install PyTorch with the CSE204 kernel. So proceed by installing PyTorch if you wish to play with it (what follows is not graded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4e0858acce7cc6ba3f62b1d9f49c5da",
     "grade": false,
     "grade_id": "cell-5b49ecc1f708214d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch import nn, optim, no_grad\n",
    "except ModuleNotFoundError:\n",
    "    !pip install torch torchvision\n",
    "    from torch import nn, optim, no_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac09201f766b93b0f3a92dfa7845e1e7",
     "grade": false,
     "grade_id": "cell-4edc051932641d80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import float as torch_float\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b1bcc84fd21a68e9ef7848d206e7261",
     "grade": false,
     "grade_id": "cell-67ea98f77f40cea8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Re-download the MNIST dataset with the torch API / format (may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9266d923b47d46a772a94cc6726d5ad0",
     "grade": false,
     "grade_id": "cell-1a622c25bcea94a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c39926a134ed3d02bf4fdc1b21183423",
     "grade": false,
     "grade_id": "cell-caeccfe11d18a90f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Create data loaders (iterators over the training and testing data, think `range`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ab8d35b883ae3b9d161e31efae8729d",
     "grade": false,
     "grade_id": "cell-522f014087e0bda1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e81ea3059b8c871f99f6a6cd2ba45a98",
     "grade": false,
     "grade_id": "cell-bc450847658afa79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Create the model as a class inheriting from nn.Module.\n",
    "You must at least define `__init__`, where you might want to define the structure of your network, as well as the `forward` method (which is basically `feed_forward`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ad8ee5057ff015b0d57ffba5c3740dd",
     "grade": false,
     "grade_id": "cell-a54e7019d6d16fca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Instanciate your model by instanciating an object of this new class. You still need a loss function (our `Cost`), and an optimizer (we hard-coded Gradient Descent in `Network`).\n",
    "\n",
    "Then we iterate on the training data for a given number of epochs, using a `for` loop. At each iteration, we must:\n",
    "* Perform the feedforward pass;\n",
    "* Compute the loss ($\\delta^L$);\n",
    "* Perform backpropagation by:\n",
    "    * Calculating the loss and the gradients of $W$ and $b$ for $l < L$;\n",
    "    * Update the parameters $W$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, eps=1e-07)\n",
    "model.train()  # Switch to training mode\n",
    "\n",
    "for _ in range(n_epoch):\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67be09feb4aada1b0f2a4c8745a3931f",
     "grade": false,
     "grade_id": "cell-c8aa5b3d3fb3c60d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To compute the testing loss / error, we use the iterator over the test dataset and the loss function, which we previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Switch to evaluation mode\n",
    "test_loss, correct = 0, 0\n",
    "# Do not compute the gradient\n",
    "with no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        pred = model(X)\n",
    "        test_loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch_float).sum().item()\n",
    "test_loss /= len(test_dataloader.dataset)\n",
    "correct /= len(test_dataloader.dataset)\n",
    "print(f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10d46bf098c715d3cd4d33f780c30c94",
     "grade": false,
     "grade_id": "cell-539006407770e5bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Your turn (Bonus - 2 pts)\n",
    "Play with tensorflow / keras and / or pytorch and provide your own model and training routine.\n",
    "\n",
    "Try to separate the data into train / dev / test where train is used to train the model, dev is used to evaluate the model's performance at each epoch, and test is hold out until the end of the training.\n",
    "\n",
    "You may want to test on other datasets. See [keras datasets](https://keras.io/api/datasets/) and [PyTorch datasets](https://pytorch.org/vision/stable/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c4376c3c56506cfd18a07663248c49e",
     "grade": true,
     "grade_id": "cell-082c2017a279e3f8",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copie de Lab6.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/jbscoggi/teaching/blob/master/Polytechnique/CSE204/Lab6.ipynb",
     "timestamp": 1576227403767
    }
   ]
  },
  "kernelspec": {
   "display_name": "CSE204",
   "language": "python",
   "name": "cse204"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
