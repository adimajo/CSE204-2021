{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d23c6ec9c3d93ae0efc7f73068bf0462",
     "grade": false,
     "grade_id": "cell-a25c0c792fa938b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CSE 204 Lab 13: Unsupervised Learning - Clustering\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/adimajo/CSE204-2021/master/data/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSE204-2021](https://moodle.polytechnique.fr/course/view.php?id=12838) Lab session #13\n",
    "\n",
    "J.B. Scoggins, Jesse Read, Adrien Ehrhardt, Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46bf410420b9cf4b2cbcb8d573a8e81f",
     "grade": false,
     "grade_id": "cell-9ed05ba8846d12be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you will implement two unsupervised learning algorithms to cluster data points based on similarity criteria: k-means, and spectral k-means. While libraries such as scikit-learn provide facilities that implement these algorithms, they are simple enough for you to implement with numpy alone. Before you begin, import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c96b2467626b794e60bae14ac63cb56",
     "grade": false,
     "grade_id": "cell-ad810c8a728ffb57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9518728a93caadd60931bb72545b19e6",
     "grade": false,
     "grade_id": "cell-410c51ab5d8c4bcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Datasets\n",
    "\n",
    "Throughout this lab, you will use 3 simple datasets to test your algorithms. Run the code below to visualize each dataset. As you can see, the first dataset consists of 4 normally-distributed clusters of points with equal variance. The second represents two clusters, one stretched vertically, and one horizontally. Finally, the last dataset represents 3 clusters distributed in rings. For convenience, the three datasets are placed in a list called `datasets`. In the rest of the lab, you will be asked to implement 2 clustering algorithms and run them on these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f15f4ea9b8e2a484fec669a59d9c4d6e",
     "grade": false,
     "grade_id": "cell-3a20658d8a1c2631",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a data set\n",
    "N = 120\n",
    "\n",
    "data1 = np.random.normal((0, 0), (0.5, 0.5), size=(N, 2))\n",
    "data1 = np.append(data1, np.random.normal((5,0), (0.5, 0.5), size=(N, 2)), axis=0)\n",
    "data1 = np.append(data1, np.random.normal((0,5), (0.5, 0.5), size=(N, 2)), axis=0)\n",
    "data1 = np.append(data1, np.random.normal((5,5), (0.5, 0.5), size=(N, 2)), axis=0)\n",
    "\n",
    "data2 = np.random.normal((2, 5), (0.25, 1), size=(N, 2))\n",
    "data2 = np.append(data2, np.random.normal((5, 5), (1, 0.25), size=(N, 2)), axis=0)\n",
    "\n",
    "radii = np.random.normal(0, 0.5,size=(N, 1))\n",
    "radii = np.append(radii, np.random.normal(4, 0.5,size=(2 * N, 1)), axis=0)\n",
    "radii = np.append(radii, np.random.normal(8, 0.5,size=(3 * N, 1)), axis=0)\n",
    "angles = np.random.uniform(size=(6 * N, 1)) * 2.0 * np.pi\n",
    "data3 = np.hstack([radii * np.cos(angles), radii * np.sin(angles)])\n",
    "\n",
    "datasets = [data1, data2, data3]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(datasets), figsize=(10, 3))\n",
    "for i,data in enumerate(datasets):\n",
    "    axes[i].scatter(data[:, 0], data[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1f9b7b039034e630f2bde6536a9a8d6",
     "grade": false,
     "grade_id": "cell-8fb6fd761d643f5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1: The k-Means Algorithm\n",
    "\n",
    "k-means is one of the simplest unsupervised learning algorithms that solves the well-known clustering problem. The algorithm defines an iterative process, where the following two steps are repeated at each iteration:\n",
    "1. take each instance belonging to the dataset and assign it to the nearest centroid, and\n",
    "2. re-calculate the centroids of each of the k clusters. \n",
    "Thus, the k centroids change their location step by step until nothing changes anymore.\n",
    "\n",
    "More formally, suppose that we are given a dataset $\\mathbf{X} = \\Big( \\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_n \\Big)$, where each $\\boldsymbol{x}_i \\in \\mathbb{R}^d$. The goal of the k-means algorithm is to group the data into $k$ cohesive clusters, where $k$ is an input parameter of the algorithm. **Your task is to implement this algorithm**. Algorithm 1 gives the pseudocode.\n",
    "\n",
    "___\n",
    "### Algorithm 1: k-means\n",
    "\n",
    "**Input**: The dataset $\\mathbf{X} = \\Big( \\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_n \\Big)$ (where each $\\boldsymbol{x}_i \\in \\mathbb{R}^d$) and the hyper-parameter $k$ (the number of clusters).<br>\n",
    "**Output**: Clusters assignments (the cluster assigned to each element of $\\mathbf{X}$).\n",
    "\n",
    "1. Initialize cluster centroids ${\\boldsymbol\\mu}_1, \\boldsymbol{\\mu}_2, \\ldots, \\boldsymbol{\\mu}_k$ by choosing $k$ instances of $\\mathbf{X}$ randomly;\n",
    "\n",
    "**Repeat:**\n",
    "\n",
    "2. Assign each instance $\\boldsymbol{x}_i \\in \\mathbf{X}$ to the closest centroid, i.e., $c_i = \\text{argmin}_z \\|\\boldsymbol{x}_i - \\boldsymbol{\\mu}_z\\|_2$;\n",
    "\n",
    "3. Re-compute the centroids $\\boldsymbol{\\mu}_1, \\ldots, \\boldsymbol{\\mu}_k$ of each cluster based on $\\boldsymbol{\\mu}_z = \\Big(\\sum_{\\boldsymbol{x} \\in \\mathbf{C}_z} \\boldsymbol{x}\\Big)/|\\mathbf{C}_z|$, where $\\mathbf{C}_z = \\{\\boldsymbol{x}_i | z_i = z\\}$ the $z$-th cluster with $z=1, \\ldots, k$ and $|\\mathbf{C}_z|$ the size of the $z$-th cluster.\n",
    "\n",
    "**until** Centroids do not change (convergence).\n",
    "___\n",
    "\n",
    "In the algorithm above, $k$ is a parameter of the algorithm and corresponds to the number of clusters we want to find; the cluster centroids $\\boldsymbol{\\mu}_z$ represent our current guesses for the positions of the centers of the clusters. To initialize the cluster centroids (in step 1 of the algorithm), we could choose $k$ training examples randomly (without replacement such that these are initially different), and set the cluster centroids to be equal to the values of these $k$ examples. Of course, other initialization methods are also possible, such as the [kmeans++ technique](https://en.wikipedia.org/wiki/K-means%2B%2B). To find the closest centroid, a distance (or similarity) function should be defined, and typically the Euclidean distance is used.\n",
    "\n",
    "Based on its notion of similarity, the problem of $k$-means clustering can be reduced to the problem of finding appropriate centroids. This, in turn, can be expressed as the task of minimizing the following objective function:\n",
    "$$\n",
    "     E(k) = \\sum_{z=1}^{k} \\sum_{\\boldsymbol{x}_i \\in \\mathbf{C}_z}\\| \\boldsymbol{x}_i - \\boldsymbol{\\mu}_z \\|_2^2.\n",
    "$$\n",
    "\n",
    "**Implement the two following functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d997ee6947fe47a218331dd087f7150e",
     "grade": false,
     "grade_id": "cell-69effc57b6477d10",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(X1: np.ndarray, X2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Distance Function\n",
    "    -----------------\n",
    "    Computes the Euclidean distance between two arrays of points.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A 2D n by m array where entry [i, j] is the distance \n",
    "    from the i-th point in X1 to the j-th point in X2.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc262ec5f7aab5e5311e0c8ce60e49f1",
     "grade": true,
     "grade_id": "cell-33b21836d8dc7730",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Python hints*:\n",
    "   * [`numpy.random.choice()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)\n",
    "   * [`numpy.argmin()`](https://numpy.org/doc/stable/reference/generated/numpy.argmin.html)\n",
    "   * [`numpy.min()`](https://numpy.org/doc/stable/reference/generated/numpy.amin.html)\n",
    "   * [`numpy.mean()`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f2399d3bfaffe92855a49999573b8bf",
     "grade": false,
     "grade_id": "cell-3bb79d305ce4e0c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def k_means(X: np.ndarray, k: int):\n",
    "    \"\"\"\n",
    "    k Means\n",
    "    --------\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    X : an n-by-d matrix of inputs\n",
    "    k : the number of clusters to find\n",
    "\n",
    "\n",
    "    Algorithm:\n",
    "    ----------\n",
    "\n",
    "    1. Initialize (choose) the centroids\n",
    "    2. Implement a `while` loop such that, while centroids have not changed since the last iteration:\n",
    "        - compute the distances of all points to each centroid\n",
    "        - label each point (associate it with) the nearest centroid\n",
    "        - recompute the centroids (i.e., average of points belonging to each centroid)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    Tuple (z, iters, err)\n",
    "\n",
    "    z : a 1D vector of labels of length n (e.g. z[i] = C_z means \"x_i is belongs to cluster C_z\")\n",
    "    iters : the number of iterations carried out until convergence\n",
    "    err: the error E(k)\n",
    "    \"\"\"\n",
    "    # Initialize (choose) the centroids\n",
    "    # Iterate until convergence\n",
    "    ## Compute distances: you should probably use :code:`euclidian_distance`\n",
    "    ## Label the points to their respective closest center\n",
    "    ## Compute new centroids (as the barycentre - mean of all coordinates - of the points belonging to this cluster)\n",
    "    # Calculate cost\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return z, iters, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f632ec234b4e4079b1316d11dd5f9cc",
     "grade": true,
     "grade_id": "cell-266f53d887370d8d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e531d5fcb702408106b2b36f48b3c10",
     "grade": false,
     "grade_id": "cell-a62b149919aabbb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To test your implementation, run the following code which will plot the 3 datasets, trying different values of $k$. It will display the number of iterations until convergence (along with $k$ in the title). **Comment in the following cell if we were able to recover the true data structure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85b6f4c295b5fb6b2f0169af523e15d3",
     "grade": false,
     "grade_id": "cell-a4bb80c820df2c24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, len(datasets), figsize=(10, 10))\n",
    "for i,k in enumerate([2, 3, 4]):\n",
    "    for j, data in enumerate(datasets):\n",
    "        labels, iters, err = k_means(data, k)\n",
    "        axes[i,j].scatter(data[:, 0], data[:, 1], c=labels, cmap='rainbow')\n",
    "        axes[i,j].set_title('$k=%d$, iter$=%d$' % (k, iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e3348f7d19fba062c9fea870bb510af",
     "grade": true,
     "grade_id": "cell-b298811213df10ac",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b26715fdcea95e9075386befdd473f28",
     "grade": false,
     "grade_id": "cell-1f9ecd36547f81b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To minimize the function $E(k)$, we wish to determine suitable centroids $(\\boldsymbol{\\mu}_z)_1^k$ such that, if the data is partitioned into corresponding clusters $(\\mathbf{C}_z)_1^k$, distances between data points and their closest cluster centroid become as small as possible.\n",
    "\n",
    "The convergence of the $k$-means algorithm is highly dependent on the initialization of the centroids. It may converge to a local minimum of the objective function $E(k)$. One way to overcome this problem is to execute the algorithm several times, with different initializations of the centroids. \n",
    "\n",
    "Another issue is how to determine the number of clusters ($k$) of the dataset. Intuitively, increasing $k$ without penalty will always reduce the amount of error in the resulting clustering, to the extreme case of zero error if each data point is in its own cluster (*i.e.*, when $k=n$). A method to determine $k$ is the *elbow rule*, similar to the one we used in the previous lab for determining the number of principal components to retain. The idea is to examine  and compare the error given above for a number of cluster solutions. In general, as the number of clusters increases, the *sum of squared errors* (SSE) should decrease because clusters are, by definition, smaller. A plot of the SSE against a series of sequential cluster levels (*i.e.*, different values) can be helpful here. That is, an appropriate cluster solution could be defined as the one where the reduction in SSE slows dramatically. This produces an \"elbow\" in the plot of SSE against the different values of $k$. \n",
    "\n",
    "Implement the elbow rule to find an appropriate value for $k$, as follows:\n",
    "\n",
    "1. Run k-means clustering for values of $k=1, \\ldots, 10$ (see `k_list`).\n",
    "2. For each $k$, calculate the total intra-cluster error ($E(k)$, given above, and append it to `costs`).\n",
    "3. Plot the curve of $E(k)$ vs $k$.\n",
    "4. Try to identify the location of a bend (elbow) in the plot -- this is generally considered as an indicator of the appropriate number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa0c22da82cea90e99b34702fe7226ba",
     "grade": false,
     "grade_id": "cell-c48684499f1ff542",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_error(dataset: np.ndarray):\n",
    "    costs = []\n",
    "    k_list = range(1, 11)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    plt.xlabel('$k$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b5e14c49ab9d5d1891f7d12bd443ebc",
     "grade": true,
     "grade_id": "cell-66d8eba8a39f5c5c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_error(datasets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8292160987b7052d8d24961c4e7f9c7c",
     "grade": false,
     "grade_id": "cell-6e5c333fb30cde78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Based on this plot, which value of $k$ do you retain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c84c1ceb7d7cede4975de64b5216a90b",
     "grade": true,
     "grade_id": "cell-b32ef0edd1cac641",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b88de2c478f89720ef2c8bae1e9f35e4",
     "grade": false,
     "grade_id": "cell-16c31384dd31c8fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2: Gaussian Mixture Model\n",
    "\n",
    "Suppose you observe and measure a number of beetles, recording their length in centimeters -- given below in the array `x`. You are curious to investigate if you have recorded more than one species, since you believe you are studying two distinct species. You decide to use Gaussian Mixture Models on the data to elaborate on this hypothesis and gain further insight into the beetle population, in particular the distribution of their length.\n",
    "\n",
    "**Task: Implement Gaussian Mixture Models**\n",
    "\n",
    "___\n",
    "### Algorithm 2: Expectation-Maximization\n",
    "\n",
    "**Input**: The dataset $\\mathbf{X} = \\Big( {x}_1, \\dots, {x}_n \\Big)$ (where each ${x}_i \\in \\mathbb{R}$ - the generalization to $\\mathbb{R}^d$ is a bit more tedious), the hyper-parameter $k$ (the number of clusters), and the number of iterations $n_{\\text{iter}}$.<br>\n",
    "**Output**: Clusters means $\\mu = (\\mu_z)_1^k$, standard deviations $\\sigma = (\\sigma_z)_1^k$, partial memberships $r_{i, j}$ and prior probabilities $\\pi_j$ (see below).\n",
    "\n",
    "**Initialize**:\n",
    "* Randomly initialize a 2D array `r` where `r[i, z]` represents the membership of sample $i$ to cluster $z$, *i.e.* $p(z_{i, z} | x_i, \\mu_z, \\sigma_z)$. For this array to express a probability, each row must sum to 1.\n",
    "* Randomly initialize a 2D array `m` where `m[:, z]` represents the mean of the gaussian of cluster $z$, *e.g.* to 0 or to the mean of each coordinate.\n",
    "* Randomly initialize a 2D array `s` where `s[:, z]` represents the standard deviation of the gaussian of cluster $z$, *e.g.* to 1 or to the standard deviation of each coordinate.\n",
    "* Randomly initialize a 1D array `w` where `w[z]` represents the prior probability of belonging to cluster $z$, *i.e.* $p(z)$, *e.g.* to $1 / k$. For this array to express a probability, it must sum to 1.\n",
    "\n",
    "**Repeat for `n_iter` iterations:**\n",
    "\n",
    "E. Compute `r[i, z]` as $\\mathbb{E}[z_i | x_i]$ by computing $\\tilde{r}_{i, z} = \\mathcal{N} \\left( x_i | \\mu_z, \\sigma_z \\right) \\pi_z$ and then normalize $r_{i, z} = \\dfrac{\\tilde{r}_{i, z}}{\\sum_{z'} \\tilde{r}_{i, z'}}$. The pdf of the Gaussian distribution is given in `g` below.\n",
    "\n",
    "M. Update the parameters `m[:, z]`, `s[:, z]` and `w[z]` using:\n",
    "* $\\mu_z = \\dfrac{\\sum_{i=1}^n r_{i, z} x_i}{\\sum_{i=1}^n r_{i, z}}$;\n",
    "* $\\sigma_z = \\dfrac{\\sum_{i=1}^n r_{i, z} (x_i - \\mu_z)^2}{\\sum_{i=1}^n r_{i, z}}$;\n",
    "* $\\pi_z = \\dfrac{\\sum_{i=1}^n r_{i, z}}{n}$.\n",
    "___\n",
    "\n",
    "\n",
    "*Note:* Plotting code is provided.\n",
    "\n",
    "*Hint:* See the [lecture slides](https://moodle.polytechnique.fr/pluginfile.php/477776/mod_resource/content/3/Slides_13c.pdf) and [notes](https://moodle.polytechnique.fr/pluginfile.php/477768/mod_resource/content/4/Slides_13c.pdf) regarding pseudocode and outline. Note that you may obtain slightly different results each time you run the algorithm due to the random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae8209b47d85fb043e328539d395a82f",
     "grade": false,
     "grade_id": "cell-3dee5fa76d753073",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# beetle lengths\n",
    "x = np.array([1.57, 1.16, 1.30, 0.26, 0.20, 0.43, 0.48, 0.34, 0.44, 0.61, 1.80, 1.40, 1.10, 1.25, 1.69], \n",
    "             dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef17071516f53e41c1d5d32c93b0abaf",
     "grade": false,
     "grade_id": "cell-616dfe3d02159734",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09137e090c1f9435f0beec34757f1277",
     "grade": false,
     "grade_id": "cell-e36b509789036809",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def g(x: float, m: float = 0.0, s: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    Gaussian probability density function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    x : a float to evaluate the pdf\n",
    "    m : the mean\n",
    "    s : the standard deviation\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    exp^(-(x-m)² / (2s)²) / sqrt(2s\\pi)\n",
    "    \"\"\"\n",
    "    return np.exp(- (x - m) ** 2 / (2 * s)) / np.sqrt(2 * s * np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "813cf8ba1924f72ad4c4252f7a1ed4b6",
     "grade": false,
     "grade_id": "cell-dac5ee4690733241",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gmm(X: np.ndarray, k: int, n_iter: int):\n",
    "    \"\"\"\n",
    "    Gaussian Mixture Model\n",
    "    ----------------------\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    X : a vector of n samples\n",
    "    k : the number of clusters to find\n",
    "    n_iter : the number of iterations to perform\n",
    "\n",
    "\n",
    "    Algorithm:\n",
    "    ----------\n",
    "\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    Tuple (z, iters, err)\n",
    "\n",
    "    z : a 1D vector of labels of length n (e.g. z[i] = c_j means \"x_i is belongs to cluster c_j\")\n",
    "    iters : the number of iterations carried out until convergence\n",
    "    err: the error E(k)\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    # Iterate for n_iter iterations\n",
    "    ## E: expected membership | params\n",
    "    ## M: maximize likelihood of this membership with params\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return m, s, r, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69e3a2038a22c8b5841955db2a744f84",
     "grade": false,
     "grade_id": "cell-e071bf1e840d231b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "m, s, r, w = gmm(x, 2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87cae8d7c0760bd1334125b062b855b5",
     "grade": false,
     "grade_id": "cell-2c6b8574c35bc7b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "# points to plot the gaussians obtained after EM\n",
    "xx = np.linspace(0, 2, num=100)\n",
    "\n",
    "# first gaussian\n",
    "plt.plot(xx, g(xx, m[0], s[0]), 'r-', label=\"$g_1$\")\n",
    "# second gaussian\n",
    "plt.plot(xx, g(xx, m[1], s[1]), 'b-', label=\"$g_2$\")\n",
    "# gaussian mixture - dashed\n",
    "plt.plot(xx, w[0] * g(xx, m[0], s[0]) + w[1] * g(xx, m[1], s[1]), 'm:', label=\"mixture\")\n",
    "\n",
    "# labels for the two bee species: the most probable class Z\n",
    "y = np.argmax(r, axis=1)\n",
    "\n",
    "plt.legend()\n",
    "plt.scatter(x, np.zeros(x.shape[0]), c=y, label=\"beetles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3545c31e1d82284306e7a305632d027c",
     "grade": true,
     "grade_id": "cell-7e2a4691bc529ea3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77d3ee33c004159d554c8c8086b7c95e",
     "grade": false,
     "grade_id": "cell-d6f29a6678f89021",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Bonus : implement a generalization to $\\mathbb{R}^d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "687b0d0c69243e5ea6cc473c8ddd2806",
     "grade": true,
     "grade_id": "cell-11f8bcf5952ee211",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a81ee3625d2dafb6ad45fd884c9a3a3e",
     "grade": false,
     "grade_id": "cell-a31d9197f0a12b42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 3: Spectral Clustering\n",
    "\n",
    "Spectral clustering techniques make use of the *spectrum* (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists in a quantitative assessment of the relative similarity of each pair of points in the dataset.\n",
    "\n",
    "Given a set of data points $\\boldsymbol{x}_1, \\ldots ,\\boldsymbol{x}_n, \\forall \\boldsymbol{x}_i \\in \\mathbb{R}^d$ and some notion of similarity $s_{ij}$ between all pairs of data points $\\boldsymbol{x}_i$ and $\\boldsymbol{x}_j$, the intuitive goal of clustering is to divide the data points into several groups such that points in the same group are similar and points in different groups are dissimilar to each other. If we do not have more information than similarities between data points, a nice way of representing the data is in form of the similarity graph $G = (V, E)$. Each vertex $v_i$ in this graph represents a data point $\\boldsymbol{x}_i$. Two vertices are connected if the similarity $s_{ij}$ between the corresponding data points $\\boldsymbol{x}_i$ and $\\boldsymbol{x}_j$ is positive or larger than a certain threshold, and the edge is weighted by $s_{ij}$. The problem of clustering can now be reformulated using the similarity graph: we want to find a partition of the graph such that the edges between different groups have very low weights (which means that points in different clusters are dissimilar from each other) and the edges within a group have high weights (which means that points within the same cluster are similar to each other).\n",
    "\n",
    "### Creating the similarity graph\n",
    "\n",
    "There are several popular constructions to transform a given set $\\boldsymbol{x}_1, \\ldots , \\boldsymbol{x}_n, \\forall \\boldsymbol{x}_i \\in \\mathbb{R}^d$ of data points with pairwise\n",
    "similarities $w_{ij}$ or pairwise distances $d_{ij}$ into a graph. When constructing similarity graphs the goal is to model the local neighborhood relationships between the data points. We will use the approach of the $k$-Nearest Neighbors graph. Here the goal is to connect vertex $\\boldsymbol{x}_i$ with vertex $\\boldsymbol{x}_j$ if $\\boldsymbol{x}_j$ is among the $k$-nearest neighbors of $\\boldsymbol{x}_i$. This definition leads to a directed graph, as the neighborhood relationship is not symmetric. The most common way to deal with this, is to simply ignore the directions of the edges; that is, we connect $\\boldsymbol{x}_i$ and $\\boldsymbol{x}_j$ with an undirected edge if $\\boldsymbol{x}_i$ is among the $k$-nearest neighbors of $\\boldsymbol{x}_j$ or if $\\boldsymbol{x}_j$ is among the $k$-nearest neighbors of $\\boldsymbol{x}_i$ (with weights $w_{ij} = w_{ji} = 1$). The resulting graph is what is usually called the $k$-nearest neighbors graph.\n",
    "\n",
    "### The algorithm\n",
    "\n",
    "The pseudocode of the spectral clustering algorithm is given as follows. In spectral clustering, the data is projected into a lower-dimensional space (the spectral/eigenvector domain) where they are easily separable, say using $k$-means.\n",
    "\n",
    "Given a dataset $\\mathbf{X}=\\{ \\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_n\\}$, where  each $\\boldsymbol{x}_i \\in \\mathbb{R}^d$ and hyper-parameter $k$:\n",
    "\n",
    "1. Construct the similarity graph $G$ as described above. Let\n",
    "    * $\\mathbf{W} = (w_{ij})_1^n$ be the adjacency matrix of this graph.\n",
    "    * $\\mathbf{D}$ be the diagonal degree matrix of graph $G$, ie $\\mathbf{D}_{ii} = \\sum_j {w}_{ij}$ and $\\mathbf{D}_{ij} = 0$ for $i \\neq j$.\n",
    "2. Compute the Laplacian matrix $\\mathbf{L} = \\mathbf{D} - \\mathbf{W}$.\n",
    "3. Apply[ eigenvalue decomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) to the Laplacian matrix $\\mathbf{L}$.\n",
    "4. Select the eigenvectors which correspond to $k$ smallest eigenvalues, which we denote by $\\mathbf{U}$.\n",
    "5. Apply $k$-means to $\\mathbf{U}$ (as if the rows were instances / samples), thus finding clusters $\\mathbf{C}_1, \\mathbf{C}_2, \\ldots, \\mathbf{C}_k$.\n",
    "\n",
    "*Hint*: Use the `euclidian_distance` function and the `k_means` implementation you wrote in Task 1.\n",
    "\n",
    "*Python hints:*\n",
    "\n",
    "* to find the `k` nearest neighbors of each $\\boldsymbol{x}_i \\in \\mathbf{X}$, you can use the function [`np.argsort`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html) with the array that stores `d(x_i, x_j)` for $\\boldsymbol{x}_i, \\boldsymbol{x}_j \\in \\mathbf{X}$;\n",
    "* you can use [`eigenvalues, eigenvectors = np.linalg.eig(L)`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html) to obtain the eigenvalues and eigenvectors of a (square) matrix `L`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c54c21b5a781dba54002bb9d05ce90af",
     "grade": false,
     "grade_id": "cell-0178318004c9d59a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def spectral_k_means(X, k, k_nn):\n",
    "    \"\"\"\n",
    "    Spectral Clustering\n",
    "    -------------------\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X : the data\n",
    "    k : the number of clusters to find\n",
    "    k_nn : the number of k nearest-neighbours to consider in the graph construction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    Similar to the `k_means` method above\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4edd686c7f668ad5eb7cc4648968a7cf",
     "grade": true,
     "grade_id": "cell-05f38d7dd2a404b3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4961bcbff45c2de8244a71b38fe0bfc",
     "grade": false,
     "grade_id": "cell-6ab3ebdcd041c832",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To test your implementation, run the following code which will plot the 3 datasets, trying different values of $k$. It will display the number of iterations (done by $k$-means) until convergence (along with $k$ in the title).\n",
    "\n",
    "Comment in the following cell if we were able to recover the true data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "567ee9046cd931b01969571782f93699",
     "grade": false,
     "grade_id": "cell-75c684606e463cf7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, len(datasets), figsize=(10, 10))\n",
    "for i, k in enumerate([2, 3, 4]):\n",
    "    for j, data in enumerate(datasets):\n",
    "        print(f\"k = {k}, dataset #{j}\")\n",
    "        labels, iters, err = spectral_k_means(data, k, 10)\n",
    "        axes[i,j].scatter(data[:, 0], data[:, 1], c=labels, cmap='rainbow')\n",
    "        axes[i,j].set_title('$k=%d$, iter$=%d$' % (k, iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f02ab5c680445bb4cff4c2422b78fab0",
     "grade": true,
     "grade_id": "cell-5088a0c8095eb085",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSE204",
   "language": "python",
   "name": "cse204"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
